{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNQO4/2aJRLuXFNf6k34Xup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hillascher5/nlp-tweets-sentiment-analysis/blob/main/Fix_bug_in_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Needed for Google Colab\n",
        "# !pip install --quiet evaluate transformers optuna datasets nltk scikit-learn\n",
        "# !pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "-xk4cCNR-SFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from optuna.pruners import MedianPruner\n",
        "from scipy.stats import pearsonr\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import glob\n",
        "import random\n",
        "import nltk\n",
        "import evaluate\n",
        "import transformers\n",
        "import torch\n",
        "import optuna\n",
        "\n",
        "USE_WANDB = True  # False when running without wandb\n",
        "SAVE_OPTUNA_DB = False\n",
        "SAVE_BEST_MODELS = True\n",
        "\n",
        "num_train_samples = 5000\n",
        "\n",
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    wandb.login()\n",
        "\n",
        "    os.environ[\"WANDB_PROJECT\"] = f\"tweet-sentiment-classification_{num_train_samples}_samples_optuna_roberta_bug_fix\"\n",
        "    os.environ[\"WANDB_INIT_TIMEOUT\"] = \"180\"\n",
        "else:\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "y3TBXWmj-VP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed_all(42)"
      ],
      "metadata": {
        "id": "nrGse1jC-XVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_train.csv', encoding='latin1')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_test.csv', encoding='latin1')"
      ],
      "metadata": {
        "id": "5G49lBEo-at_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and shuffle for better stratified splits\n",
        "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
        "df_full = df_full.sample(frac=1.0, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "B_JD3EkS-bPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try minimal pre-processing\n",
        "def light_preprocess(text):\n",
        "    return text.strip()                             # Remove unnecessary spaces\n",
        "\n",
        "is_preprocessed = \"minimal_preprocess\"\n",
        "df_full[\"clean_text\"] = df_full[\"OriginalTweet\"].apply(light_preprocess)"
      ],
      "metadata": {
        "id": "7d3QYnYk-c22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sentiments to unique numeric IDs\n",
        "unique_labels = sorted(df_full[\"Sentiment\"].unique())\n",
        "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "df_full[\"label\"] = df_full[\"Sentiment\"].map(label2id)"
      ],
      "metadata": {
        "id": "IfoEgqxo-fA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified split: 70% train, 15% val, 15% test\n",
        "train_val_df, test_df = train_test_split(df_full, test_size=0.15, stratify=df_full[\"label\"], random_state=42)\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.1765, stratify=train_val_df[\"label\"], random_state=42)\n",
        "\n",
        "# Confirm sizes\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n",
        "print(\"Test size:\", len(test_df))"
      ],
      "metadata": {
        "id": "AzD6ybt8-hJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset_df, _ = train_test_split(\n",
        "    train_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=num_train_samples,\n",
        "    stratify=train_df[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_subset_df, _ = train_test_split(\n",
        "    val_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=500,\n",
        "    stratify=val_df[\"label\"],\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "pupeiYqd-jZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose pretrained models\n",
        "roberta_model_name = \"roberta-base\"\n",
        "\n",
        "sentiment_labels = df_full['Sentiment'].unique()\n",
        "n_labels = len(sentiment_labels)\n",
        "\n",
        "# Load RoBERTa tokenizer and model\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name, num_labels=n_labels)"
      ],
      "metadata": {
        "id": "2TYK14N7-mOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function_roberta(examples):\n",
        "    return roberta_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=256)"
      ],
      "metadata": {
        "id": "8uTB5WoC-sRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to Hugging Face Dataset\n",
        "hf_subset_train = Dataset.from_pandas(train_subset_df)\n",
        "hf_subset_val = Dataset.from_pandas(val_subset_df)\n",
        "\n",
        "hf_train = Dataset.from_pandas(train_df[[\"clean_text\", \"label\"]])\n",
        "hf_val = Dataset.from_pandas(val_df[[\"clean_text\", \"label\"]])\n",
        "hf_test = Dataset.from_pandas(test_df[[\"clean_text\", \"label\"]])"
      ],
      "metadata": {
        "id": "zCe5Ky_J-upm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize subsets\n",
        "# Tokenize for RoBERTa\n",
        "tokenized_roberta_train_sub = hf_subset_train.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_train_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_val_sub = hf_subset_val.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_val_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_train = hf_train.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_val = hf_val.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_test = hf_test.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "zNVbw8W7-vLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1_macro\": f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"],\n",
        "        \"precision_macro\": precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"],\n",
        "        \"recall_macro\": recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "aLzqUx1k-2w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Load existing study from the SQLite file\n",
        "db_path = \"/content/drive/MyDrive/Colab Notebooks/nlp_project/optuna/roberta_study_stratify_maxl_256_minimal_preprocess_5000_samples_optuna.db\"\n",
        "study_name = \"roberta_study_stratify_minimal_preprocess\"\n",
        "\n",
        "study_roberta = optuna.load_study(\n",
        "    study_name=study_name,\n",
        "    storage=f\"sqlite:///{db_path}\"\n",
        ")\n",
        "\n",
        "best_trial_roberta = study_roberta.best_trial\n",
        "best_params_roberta = best_trial_roberta.params\n",
        "print(\"Loaded best RoBERTa params:\", best_params_roberta)"
      ],
      "metadata": {
        "id": "WMUe99eK-4wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_trainer(model_checkpoint, trial, run_prefix, train_dataset, val_dataset, report_to=\"none\"):\n",
        "    # Sample hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    num_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
        "    n_samples = len(train_dataset)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=5)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    run_name = f\"{run_prefix}-ep{num_epochs}-lr{learning_rate}-bs{batch_size}-samples{n_samples}-run{int(time.time())}\"\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"./results/{run_prefix}/{run_name}\",\n",
        "        disable_tqdm=True,\n",
        "        fp16=True,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=0.01,\n",
        "        label_smoothing_factor=0.1,\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy=\"epoch\",\n",
        "        logging_dir=f\"./logs/{run_prefix}/{run_name}\",\n",
        "        run_name=run_name,\n",
        "        report_to=report_to,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "NVKsi070A4uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QAvOD1K-Mqr"
      },
      "outputs": [],
      "source": [
        "best_params_roberta = best_trial_roberta.params\n",
        "run_name_roberta = f\"roberta_final_stratify_{is_preprocessed}-ep{best_params_roberta['num_train_epochs']}-lr{best_params_roberta['learning_rate']:.1e}-bs{best_params_roberta['batch_size']}\"\n",
        "if USE_WANDB:\n",
        "  wandb.init(project=f\"tweet-sentiment-classification-best_{num_train_samples}_samples_optuna_roberta_bug_fix\", name=run_name_roberta, reinit=True)\n",
        "\n",
        "final_trainer_roberta = build_trainer(\n",
        "    model_checkpoint=\"roberta-base\",\n",
        "    trial=best_trial_roberta,\n",
        "    run_prefix=f\"roberta_final_stratify_{is_preprocessed}\",\n",
        "    train_dataset=tokenized_roberta_train,\n",
        "    val_dataset=tokenized_roberta_val,\n",
        "    report_to=\"wandb\" if USE_WANDB else \"none\"\n",
        ")\n",
        "final_trainer_roberta.train()\n",
        "final_trainer_roberta.evaluate(tokenized_roberta_test)\n",
        "if USE_WANDB:\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir_best = f\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer\"\n",
        "\n",
        "if SAVE_BEST_MODELS:\n",
        "  final_trainer_roberta.save_model(f\"{save_dir_best}/roberta_fixed_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna\")\n",
        "  roberta_tokenizer.save_pretrained(f\"{save_dir_best}/roberta_fixed_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna\")\n",
        "  !cp -r {save_dir_best}roberta_fixed_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna \"{save_dir_best}/roberta_fixed_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n"
      ],
      "metadata": {
        "id": "Etub890zBAnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSDmXnNPFm6A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}