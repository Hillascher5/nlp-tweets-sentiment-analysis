{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNDKmmrAr9t9UJv7BNSUFZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hillascher5/nlp-tweets-sentiment-analysis/blob/main/DeBerta_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try another model for the task"
      ],
      "metadata": {
        "id": "QuMksGtXPH5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d5n6iVnKO8GJ"
      },
      "outputs": [],
      "source": [
        "# # Needed for Google Colab\n",
        "# !pip install --quiet evaluate transformers optuna datasets nltk scikit-learn\n",
        "# !pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from optuna.pruners import MedianPruner\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import glob\n",
        "import nltk\n",
        "import evaluate\n",
        "import transformers\n",
        "import torch\n",
        "import optuna\n",
        "import wandb\n",
        "wandb.login()\n",
        "# API key - 0cbd7fe3cffd71df993b30edb4fa0db94f114413\n",
        "\n",
        "num_train_samples = 5000\n",
        "num_train_samples_str = str(num_train_samples)\n",
        "os.environ[\"WANDB_PROJECT\"] = f\"tweet-sentiment-classification_split_to_test_maxl_128_deberta_{num_train_samples_str}_samples_optuna\"\n",
        "os.environ[\"WANDB_INIT_TIMEOUT\"] = \"180\"\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "aqmJFcuDPS8b",
        "outputId": "01601d81-1bc0-45aa-bb25-25288a1f5f55"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhillas\u001b[0m (\u001b[33mhillas-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_train.csv', encoding='latin1')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_test.csv', encoding='latin1')"
      ],
      "metadata": {
        "id": "TFMan1IiPYUj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and shuffle for better stratified splits\n",
        "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
        "df_full = df_full.sample(frac=1.0, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "uvgzCWKxPbj1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try without pre-processing\n",
        "is_preprocessed = \"no_preprocess\"\n",
        "df_full[\"clean_text\"] = df_full[\"OriginalTweet\"]"
      ],
      "metadata": {
        "id": "-kKoOBvqPd58"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sentiments to unique numeric IDs\n",
        "unique_labels = sorted(df_full[\"Sentiment\"].unique())\n",
        "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "df_full[\"label\"] = df_full[\"Sentiment\"].map(label2id)"
      ],
      "metadata": {
        "id": "ENXyMQPsPjVq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified split: 70% train, 15% val, 15% test\n",
        "train_val_df, test_df = train_test_split(df_full, test_size=0.15, stratify=df_full[\"label\"], random_state=42)\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.1765, stratify=train_val_df[\"label\"], random_state=42)\n",
        "\n",
        "# Confirm sizes\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n",
        "print(\"Test size:\", len(test_df))"
      ],
      "metadata": {
        "id": "qt_6tFsKPpwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset_df, _ = train_test_split(\n",
        "    train_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=num_train_samples,\n",
        "    stratify=train_df[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_subset_df, _ = train_test_split(\n",
        "    val_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=500,\n",
        "    stratify=val_df[\"label\"],\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "tsc8XVWiPuFD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose pretrained models\n",
        "bert_model_name = \"microsoft/deberta-v3-base\"\n",
        "\n",
        "sentiment_labels = df_full['Sentiment'].unique()\n",
        "n_labels = len(sentiment_labels)\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=n_labels)"
      ],
      "metadata": {
        "id": "MshYxTH3Pge7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize function\n",
        "def tokenize_function_bert(examples):\n",
        "    return bert_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=128)"
      ],
      "metadata": {
        "id": "kAf36TYSPv6G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to Hugging Face Dataset\n",
        "hf_subset_train = Dataset.from_pandas(train_subset_df)\n",
        "hf_subset_val = Dataset.from_pandas(val_subset_df)\n",
        "\n",
        "hf_train = Dataset.from_pandas(train_df[[\"clean_text\", \"label\"]])\n",
        "hf_val = Dataset.from_pandas(val_df[[\"clean_text\", \"label\"]])\n",
        "hf_test = Dataset.from_pandas(test_df[[\"clean_text\", \"label\"]])"
      ],
      "metadata": {
        "id": "QvtxTOhlPyob"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize subsets\n",
        "# Tokenize for BERT\n",
        "tokenized_bert_train_sub = hf_subset_train.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_train_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_bert_val_sub = hf_subset_val.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_val_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "_0yGNMG8P1OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize full dataset\n",
        "# Tokenize for BERT\n",
        "tokenized_bert_train = hf_train.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_bert_val = hf_val.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_bert_test = hf_test.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "23Qs5lyvP3Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1_macro\": f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "hnFwcxffP5ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_trainer(model_checkpoint, trial, run_prefix, train_dataset, val_dataset):\n",
        "    # Sample hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    num_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
        "    n_samples = len(train_dataset)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=5)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    run_name = f\"{run_prefix}-ep{num_epochs}-lr{learning_rate}-bs{batch_size}-samples{n_samples}-run{int(time.time())}-{is_preprocessed}\"\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"./results/{run_prefix}/{run_name}\",\n",
        "        disable_tqdm=True,\n",
        "        fp16=True,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=0.01,\n",
        "        label_smoothing_factor=0.1,\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy=\"epoch\",\n",
        "        logging_dir=f\"./logs/{run_prefix}/{run_name}\",\n",
        "        run_name=run_name,\n",
        "        report_to=\"wandb\",\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "FgJWGgSkP7oa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_bert(trial):\n",
        "    trainer = build_trainer(\n",
        "        model_checkpoint=\"microsoft/deberta-v3-base\",\n",
        "        trial=trial,\n",
        "        run_prefix=\"deberta\",\n",
        "        train_dataset=tokenized_bert_train_sub,\n",
        "        val_dataset=tokenized_bert_val_sub\n",
        "    )\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    return eval_result[\"eval_f1_macro\"]"
      ],
      "metadata": {
        "id": "wcW-klOTP_LB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study_bert = optuna.create_study(direction=\"maximize\",\n",
        "                                 pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
        "                                 study_name=f\"deberta_study_stratify_{is_preprocessed}\",\n",
        "                                 storage=f\"sqlite:////content/drive/MyDrive/Colab Notebooks/nlp_project/optuna/deberta_study_stratify_maxl_128_{is_preprocessed}_{num_train_samples_str}_samples_optuna.db\",\n",
        "                                 load_if_exists=True)\n",
        "study_bert.optimize(objective_bert, n_trials=5)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "lIn83SWhQBbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_bert = study_bert.best_trial\n",
        "print('DeBerta best trial on subset:')\n",
        "print(best_trial_bert.params)"
      ],
      "metadata": {
        "id": "9dpxPyrFQDvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_bert = best_trial_bert.params\n",
        "run_name_bert = f\"deberta_final_stratify_{is_preprocessed}-ep{best_params_bert['num_train_epochs']}-lr{best_params_bert['learning_rate']:.1e}-bs{best_params_bert['batch_size']}\"\n",
        "wandb.init(project=f\"tweet-sentiment-classification_split_to_test_maxl_128_deberta_{num_train_samples_str}_samples_optuna\", name=run_name_bert, reinit=True)\n",
        "\n",
        "final_trainer_bert = build_trainer(\n",
        "    model_checkpoint=\"microsoft/deberta-v3-base\",\n",
        "    trial=best_trial_bert,\n",
        "    run_prefix=f\"deberta_final_stratify_{is_preprocessed}\",\n",
        "    train_dataset=tokenized_bert_train,\n",
        "    val_dataset=tokenized_bert_val\n",
        ")\n",
        "final_trainer_bert.train()\n",
        "final_trainer_bert.evaluate(tokenized_bert_test)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "t0s91u4cQJjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_trainer_bert.save_model(f\"models/w_test_split/deberta_final_stratify_{is_preprocessed}_{num_train_samples_str}_samples_optuna\")\n",
        "bert_tokenizer.save_pretrained(f\"models/w_test_split/deberta_final_stratify_{is_preprocessed}_{num_train_samples_str}_samples_optuna\")\n",
        "!cp -r models/w_test_split/deberta_final_stratify_{is_preprocessed}_{num_train_samples_str}_samples_optuna \"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/deberta_best_model_stratify_maxl_128_{is_preprocessed}_{num_train_samples_str}_samples_optuna\""
      ],
      "metadata": {
        "id": "gHUvBjW0QOkk"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}