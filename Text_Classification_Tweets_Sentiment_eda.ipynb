{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hillascher5/nlp-tweets-sentiment-analysis/blob/main/Text_Classification_Tweets_Sentiment_eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd1db53d",
      "metadata": {
        "id": "fd1db53d"
      },
      "source": [
        "## **NLP - Text Classification Project**\n",
        "Group H - August 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e39c64d",
      "metadata": {
        "id": "0e39c64d"
      },
      "source": [
        "Classification of tweets from Twitter that have been manually tagged for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Needed for Google Colab\n",
        "# !pip install --quiet evaluate transformers optuna datasets nltk scikit-learn\n",
        "# !pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "MdWj5rFhiNYR"
      },
      "id": "MdWj5rFhiNYR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b425b0",
      "metadata": {
        "id": "01b425b0"
      },
      "outputs": [],
      "source": [
        "# %env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import glob\n",
        "import nltk\n",
        "# import evaluate\n",
        "import transformers\n",
        "# import optuna\n",
        "import wandb\n",
        "# wandb.login()\n",
        "\n",
        "# os.environ[\"WANDB_PROJECT\"] = \"tweet-sentiment-classification\"\n",
        "# os.environ[\"WANDB_INIT_TIMEOUT\"] = \"180\"\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "862fa5f6",
      "metadata": {
        "id": "862fa5f6"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df_train = pd.read_csv('data/Corona_NLP_train.csv', encoding='latin1')\n",
        "df_val = pd.read_csv('data/Corona_NLP_test.csv', encoding='latin1')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1c82ca",
      "metadata": {
        "id": "ce1c82ca"
      },
      "source": [
        "### *Data Visualization*\n",
        "Visualize raw data to understand data distributions, explore frequent words, find outliers or imbalances in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "921d53f6",
      "metadata": {
        "id": "921d53f6"
      },
      "source": [
        "**Sentiment Label Distribution**\n",
        "\n",
        "Visualizaion of sentiment labels distribution in both the training and validation sets. This helped to quickly identify potential class imbalances. Then calculation of the percentage of each sentiment to quantitatively assess how balanced the data is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b20710",
      "metadata": {
        "id": "a2b20710"
      },
      "outputs": [],
      "source": [
        "## Visualize distribution of sentiment labels\n",
        "# Find all unique labels and reorder them for the bar plot\n",
        "sentiment_labels = df_train['Sentiment'].unique()\n",
        "labels_new_order = ['Extremely Positive','Positive','Neutral','Negative','Extremely Negative']\n",
        "\n",
        "# Plot distribution\n",
        "sns.countplot(data=df_train, x='Sentiment', order=labels_new_order)\n",
        "plt.title(\"Distribution of Sentiment Labels Train Set\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "sns.countplot(data=df_val, x='Sentiment', order=labels_new_order)\n",
        "plt.title(\"Distribution of Sentiment Labels Validation Set\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c99664",
      "metadata": {
        "id": "27c99664"
      },
      "outputs": [],
      "source": [
        "# Show labels percentages - examine if data is imbalanced\n",
        "label_counts_train = df_train['Sentiment'].value_counts()\n",
        "label_percentages_train = df_train['Sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "label_counts_val = df_val['Sentiment'].value_counts()\n",
        "label_percentages_val = df_val['Sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Display percentages\n",
        "print(\"Label Distribution Train(%):\")\n",
        "print(label_percentages_train.round(2))\n",
        "\n",
        "print(\"\\nLabel Distribution Validation(%):\")\n",
        "print(label_percentages_val.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bac9dd0",
      "metadata": {
        "id": "2bac9dd0"
      },
      "source": [
        "**Finding Junk Locations**\n",
        "\n",
        "After reviewing the location data, it became evident that many entries were either ambiguous, non-geographic, or entirely fictional. To prevent these from skewing the analysis, such locations were identified and tagged as \"unknown\".\n",
        "\n",
        "Known junk patterns (e.g., \"earth\", \"everywhere\" etc.) were filtered using keyword matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fcd3b3a",
      "metadata": {
        "id": "9fcd3b3a"
      },
      "outputs": [],
      "source": [
        "# Junk keywords to catch broad cases\n",
        "junk_keywords = [\n",
        "        'earth', 'everywhere', 'nowhere', 'somewhere', 'your heart', 'in your', 'here','kitchen',\n",
        "        'dreams', 'global', 'anywhere', 'the world', 'internet', 'mars', 'moon', 'sun', 'planet',\n",
        "        'universe', 'home', 'outer space', 'narnia', 'hogwarts', 'mordor', 'where', 'location',\n",
        "        'matrix', 'heaven', 'hell', 'bed', 'tatooine', 'gotham', 'vagabond', 'there',\n",
        "        'middle of nowhere', 'lost', 'cyberspace', 'nan'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a15e78",
      "metadata": {
        "id": "20a15e78"
      },
      "outputs": [],
      "source": [
        "# Count how many rows match the keyword-based junk filter\n",
        "def find_junk(df, set_name):\n",
        "    junk_by_keyword = df['Location'].astype(str).str.lower().apply(\n",
        "        lambda loc: any(keyword in loc for keyword in junk_keywords)\n",
        "    ).sum()\n",
        "\n",
        "    # Display junk count\n",
        "    print(f\"Junk matched by keywords for {set_name} set: {junk_by_keyword}\")\n",
        "\n",
        "    # Calculate percentage\n",
        "    junk_percentage = round((junk_by_keyword / len(df_train)) * 100, 2)\n",
        "    print(f\"Percentage of junk locations for {set_name} set: {junk_percentage}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63260efa",
      "metadata": {
        "id": "63260efa"
      },
      "outputs": [],
      "source": [
        "find_junk(df_train, 'train')\n",
        "find_junk(df_val, 'validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3501e942",
      "metadata": {
        "id": "3501e942"
      },
      "source": [
        "#### Filtering Junk Locations\n",
        "\n",
        "To improve location accuracy, we defined a function that filters out junk entries. It marks any location as `'unknown'` if it contains:\n",
        "\n",
        "- Keywords from the predefined junk list\n",
        "- Digits (e.g., coordinates, postal codes)\n",
        "- Unwanted punctuation (excluding commas)\n",
        "\n",
        "The cleaned and filtered results are stored in a new column, allowing further processing to focus only on meaningful locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a546e7b",
      "metadata": {
        "id": "3a546e7b"
      },
      "outputs": [],
      "source": [
        "def mark_junk_locations(df, junk_list, location_col='Location', new_col='Location_cleaned'):\n",
        "    def is_junk(loc):\n",
        "        loc = str(loc).strip().lower()\n",
        "\n",
        "        # Check for junk keywords\n",
        "        if any(keyword in loc for keyword in junk_list):\n",
        "            return True\n",
        "\n",
        "        # Check for digits\n",
        "        if any(char.isdigit() for char in loc):\n",
        "            return True\n",
        "\n",
        "        # Check for punctuation (excluding comma)\n",
        "        allowed_punctuation = {','}\n",
        "        if any(char in string.punctuation and char not in allowed_punctuation for char in loc):\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    # Clean and mark\n",
        "    df[new_col] = df[location_col].astype(str).str.strip().str.lower()\n",
        "    df[new_col] = df[new_col].apply(lambda loc: 'unknown' if is_junk(loc) else loc)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e603a95",
      "metadata": {
        "id": "2e603a95"
      },
      "outputs": [],
      "source": [
        "# Apply junk cleaner\n",
        "df_train = mark_junk_locations(df_train, junk_keywords)\n",
        "df_val = mark_junk_locations(df_val, junk_keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ba0dfa",
      "metadata": {
        "id": "39ba0dfa"
      },
      "source": [
        "#### Geocoding Tweet Locations - PRE-PROCESSING\n",
        "\n",
        "Tweet locations often appear in different forms (e.g., \"london\", \"london, uk\"). To unify them, we used a geocoder (Nominatim), which grouped similar locations by mapping them to the same country. The process was done in chunks to handle rate limits and saved to CSVs for reuse — there's no need to rerun it, as it is time-consuming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a753948f",
      "metadata": {
        "id": "a753948f"
      },
      "outputs": [],
      "source": [
        "# def run_geocoder(df, set_name):\n",
        "#     # Setup geocoder\n",
        "#     geolocator = Nominatim(user_agent=\"tweet-country-mapper\")\n",
        "\n",
        "#     def get_country(location):\n",
        "#         # Handle NaNs\n",
        "#         if pd.isna(location):\n",
        "#             return None\n",
        "#         loc = str(location).strip().lower()\n",
        "#         if loc in {\"nan\", \"none\", \"\", \"unknown\", \"null\"}:\n",
        "#             return None\n",
        "\n",
        "#         try:\n",
        "#             geo = geolocator.geocode(location, language='en', timeout=10)\n",
        "#             if geo and geo.raw.get(\"display_name\"):\n",
        "#                 parts = geo.raw[\"display_name\"].split(\",\")\n",
        "#                 return parts[-1].strip()\n",
        "#         except GeocoderTimedOut:\n",
        "#             return get_country(location)\n",
        "#         except Exception:\n",
        "#             return None\n",
        "\n",
        "#     # Chunk helper\n",
        "#     def chunk_list(lst, chunk_size):\n",
        "#         for i in range(0, len(lst), chunk_size):\n",
        "#             yield lst[i:i + chunk_size]\n",
        "\n",
        "#     # Run geocoding in chunks\n",
        "#     def geocode_in_chunks(locations, chunk_size=500, delay=1.1, start_chunk=0):\n",
        "#         chunks = list(chunk_list(locations, chunk_size))\n",
        "#         for idx, chunk in enumerate(chunks[start_chunk:], start=start_chunk):\n",
        "#             print(f\"Processing chunk {idx + 1} of {len(chunks)}...\")\n",
        "\n",
        "#             result = {}\n",
        "#             for loc in tqdm(chunk):\n",
        "#                 time.sleep(delay)\n",
        "#                 result[loc] = get_country(loc)\n",
        "\n",
        "#             # Save to CSV\n",
        "#             df_part = pd.DataFrame(list(result.items()), columns=['Location', 'Country'])\n",
        "#             df_part.to_csv(f'tmp/{set_name}/geocoded_part_{idx + 1}.csv', index=False)\n",
        "\n",
        "#             print(f\"Saved geocoded_part_{idx + 1}.csv\")\n",
        "\n",
        "#     # Replace this with unique cleaned locations\n",
        "#     unique_locations = df['Location_cleaned'].dropna().unique().tolist()\n",
        "\n",
        "#     geocode_in_chunks(unique_locations, chunk_size=500, delay=1.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9e5299",
      "metadata": {
        "id": "4b9e5299"
      },
      "outputs": [],
      "source": [
        "# ## Merging all Chunks CSVs into one CSV\n",
        "# def merge_geo_csvs(set_name):\n",
        "#     # Load all parts from tmp\n",
        "#     all_parts = glob.glob(f\"tmp/{set_name}/geocoded_part_*.csv\")\n",
        "\n",
        "#     merged = pd.concat([pd.read_csv(part) for part in all_parts], ignore_index=True)\n",
        "\n",
        "#     # Save final merged file\n",
        "#     merged.to_csv(f\"geocoded_full_{set_name}_set.csv\", index=False)\n",
        "#     print(\"Saved final geocoded_full.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a236e0",
      "metadata": {
        "id": "a9a236e0"
      },
      "outputs": [],
      "source": [
        "# # Run geocoder preprocessing for train and validation sets\n",
        "# run_geocoder(df_train, 'train')\n",
        "# merge_geo_csvs('train')\n",
        "\n",
        "# run_geocoder(df_val, 'validation')\n",
        "# merge_geo_csvs('validation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96771d9d",
      "metadata": {
        "id": "96771d9d"
      },
      "source": [
        "**Map locations to countries after pre-processing**\n",
        "\n",
        "Mapping each tweet’s Location field to a standardized country using a preprocessed geocoding CSV. This allows us to group tweets by country for further analysis.\n",
        "\n",
        "Temporal analysis was performed by converting the TweetAt field to datetime format and plotting the number of tweets per sentiment over time. Additionally, sentiment distribution across the top 10 countries with the most tweets was examined to identify geographic trends in public opinion during the pandemic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be701be6",
      "metadata": {
        "id": "be701be6"
      },
      "outputs": [],
      "source": [
        "def loc_to_country_df(df, set_name):\n",
        "    # Path to where the chunk CSVs are stored\n",
        "    geocoded_df = pd.read_csv(f'data/geocoded_full_{set_name}_set.csv')\n",
        "\n",
        "    # Map location to country\n",
        "    location_to_country = dict(zip(geocoded_df['Location'], geocoded_df['Country']))\n",
        "\n",
        "    # Assign the countries to main dataframe\n",
        "    df['Country'] = df['Location_cleaned'].map(location_to_country)\n",
        "\n",
        "    # Clean temporary column\n",
        "    df.drop(columns=[\"Location_cleaned\"], inplace=True)\n",
        "\n",
        "loc_to_country_df(df_train, 'train')\n",
        "loc_to_country_df(df_val, 'validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c60b8ec",
      "metadata": {
        "id": "9c60b8ec"
      },
      "outputs": [],
      "source": [
        "## Plot sentiment counts over time and location\n",
        "# Convert date column to datetime\n",
        "df_train['TweetAt'] = pd.to_datetime(df_train['TweetAt'],format='%d-%m-%Y', errors='coerce')\n",
        "\n",
        "# Drop rows where TweetAt is missing\n",
        "df_train = df_train.dropna(subset=['TweetAt'])\n",
        "\n",
        "# Group by date and sentiment\n",
        "daily_sentiment_train = df_train.groupby(['TweetAt', 'Sentiment']).size().reset_index(name='Count')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=daily_sentiment_train, x='TweetAt', y='Count', hue='Sentiment', marker='o')\n",
        "plt.title(\"Sentiment Counts Over Time - Train Set\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Tweet Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "## By location\n",
        "# Limit to top 10 locations\n",
        "top_locations_train = df_train['Country'].value_counts().nlargest(10).index\n",
        "filtered_df_train = df_train[df_train['Country'].isin(top_locations_train)]\n",
        "\n",
        "# Group by location and sentiment\n",
        "location_sentiment_train = filtered_df_train.groupby(['Country', 'Sentiment']).size().reset_index(name='Count')\n",
        "\n",
        "# Determine sorting order based on total counts\n",
        "order = (\n",
        "    location_sentiment_train.groupby('Country')['Count']\n",
        "    .sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .index\n",
        ")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=location_sentiment_train, x='Country', y='Count', hue='Sentiment', order=order)\n",
        "plt.title(\"Sentiment Distribution by Country (Top 10) - Train Set\")\n",
        "plt.xlabel(\"Country\")\n",
        "plt.ylabel(\"Tweet Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da40b058",
      "metadata": {
        "id": "da40b058"
      },
      "source": [
        "**Tweet Length Distribution**\n",
        "\n",
        "Distribution of tweet lengths (in characters) for both the training and validation sets. This helps identify outliers, understand content variability, and ensure consistency between datasets before modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8dcd23",
      "metadata": {
        "scrolled": false,
        "id": "1f8dcd23"
      },
      "outputs": [],
      "source": [
        "# Display tweet length distribution\n",
        "df_train['TweetLength'] = df_train['OriginalTweet'].apply(len)\n",
        "df_val['TweetLength'] = df_val['OriginalTweet'].apply(len)\n",
        "\n",
        "# Plot distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df_train['TweetLength'], bins=50)\n",
        "plt.title(\"Tweet Length Distribution Train Set\")\n",
        "plt.xlabel(\"Number of Characters\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.histplot(df_val['TweetLength'], bins=50)\n",
        "plt.title(\"Tweet Length Distribution Validation Set\")\n",
        "plt.xlabel(\"Number of Characters\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5522b248",
      "metadata": {
        "id": "5522b248"
      },
      "source": [
        "**Word Clouds**\n",
        "\n",
        "Generating word clouds from the raw tweets to visualize the most frequent words in the dataset. Providing a general overview of the vocabulary used during the COVID-19 period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d42780",
      "metadata": {
        "id": "c8d42780"
      },
      "outputs": [],
      "source": [
        "## Display word cloud to visualize more frequent words in tweets\n",
        "# Create text from all tweets\n",
        "text_train = \" \".join(tweet for tweet in df_train['OriginalTweet'])\n",
        "# text_val = \" \".join(tweet for tweet in df_val['OriginalTweet'])\n",
        "\n",
        "# Generate NON-FILTERED word cloud\n",
        "wordcloud_train = WordCloud(width=800, height=400, background_color='white').generate(text_train)\n",
        "# wordcloud_val = WordCloud(width=800, height=400, background_color='white').generate(text_val)\n",
        "\n",
        "# Plot word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_train, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of All Tweets - Train Set\")\n",
        "plt.show()\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.imshow(wordcloud_val, interpolation='bilinear')\n",
        "# plt.axis('off')\n",
        "# plt.title(\"Word Cloud of All Tweets - Validation Set\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "785880d1",
      "metadata": {
        "id": "785880d1"
      },
      "source": [
        "To obtain a more meaningful visualization, we create a filtered word cloud where we remove:\n",
        "stopwords (e.g., \"the\", \"and\", \"is\"),\n",
        "common noise seen above, like \"https\", \"amp\", or \"t\",\n",
        "and domain-specific terms like \"covid\", \"coronavirus\", etc.\n",
        "\n",
        "This helps highlight non-trivial themes in the tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c864c21",
      "metadata": {
        "id": "9c864c21"
      },
      "outputs": [],
      "source": [
        "# Generate filtered word cloud - remove unmeaningful words\n",
        "def clean_text_for_wordcloud(text, exact_banned_words_set, banned_substrings_list, stopwords_set):\n",
        "   # Normalize case + strip punctuation\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    filtered_words = [\n",
        "        word for word in words\n",
        "        if word not in exact_banned_words_set\n",
        "        and not any(sub in word for sub in banned_substrings_list)\n",
        "        and word not in stopwords_set\n",
        "    ]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# Define stopwords and filters\n",
        "all_stopwords = set(STOPWORDS)\n",
        "exact_banned_words = {'t', 'co', 'https', 'amp'}\n",
        "banned_substrings = ['covid', 'corona', 'virus', 'pandemic']\n",
        "\n",
        "# Apply to texts\n",
        "text_train_clean = clean_text_for_wordcloud(text_train, exact_banned_words, banned_substrings, all_stopwords)\n",
        "# text_val_clean = clean_text_for_wordcloud(text_val, exact_banned_words, banned_substrings, all_stopwords)\n",
        "\n",
        "# Generate word clouds\n",
        "wordcloud_train_clean = WordCloud(width=800, height=400, background_color='white').generate(text_train_clean)\n",
        "# wordcloud_val_clean = WordCloud(width=800, height=400, background_color='white').generate(text_val_clean)\n",
        "\n",
        "# Plot filtered word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_train_clean, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Filtered Word Cloud of All Tweets - Train Set\")\n",
        "plt.show()\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.imshow(wordcloud_val_clean, interpolation='bilinear')\n",
        "# plt.axis('off')\n",
        "# plt.title(\"Filtered Word Cloud of All Tweets - Validation Set\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1583f6bc",
      "metadata": {
        "id": "1583f6bc"
      },
      "source": [
        "**Top Words per Sentiment**\n",
        "\n",
        "Most frequent words per sentiment analysis, after cleaning the tweets—removing stopwords, punctuation, numbers, and common domain terms like \"covid\". This highlights meaningful differences in language across Positive, Negative, and Neutral tweets, helping understand sentiment-specific word usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447bf721",
      "metadata": {
        "id": "447bf721"
      },
      "outputs": [],
      "source": [
        "# Set up stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "domain_stopwords = {'covid', 'covid19', 'coronavirus', 'pandemic', 'corona', 'virus'}\n",
        "\n",
        "# Tokenization + Cleaning Function\n",
        "def tokenize_and_clean_filtered(text):\n",
        "    words = str(text).lower().split()\n",
        "    cleaned_words = []\n",
        "\n",
        "    for word in words:\n",
        "        word = word.strip(string.punctuation)  # reassign cleaned word\n",
        "\n",
        "        if (\n",
        "            word not in stop_words\n",
        "            and word not in domain_stopwords\n",
        "            and not any(char.isdigit() for char in word)\n",
        "            and word not in punctuation\n",
        "            and len(word) > 2\n",
        "        ):\n",
        "            cleaned_words.append(word)\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "\n",
        "# Count top N words per sentiment\n",
        "def get_top_words_by_sentiment(df, text_col='OriginalTweet', label_col='Sentiment', top_n=10):\n",
        "    sentiment_top_words = {}\n",
        "\n",
        "    for sentiment in df[label_col].unique():\n",
        "        subset = df[df[label_col] == sentiment]\n",
        "        all_words = []\n",
        "        for tweet in subset[text_col]:\n",
        "            all_words.extend(tokenize_and_clean_filtered(tweet))\n",
        "\n",
        "        most_common = Counter(all_words).most_common(top_n)\n",
        "        sentiment_top_words[sentiment] = most_common\n",
        "\n",
        "    return sentiment_top_words\n",
        "\n",
        "top_words_train = get_top_words_by_sentiment(df_train)\n",
        "\n",
        "# Convert for plotting\n",
        "rows = []\n",
        "for sentiment, word_counts in top_words_train.items():\n",
        "    for word, count in word_counts:\n",
        "        rows.append({\"Sentiment\": sentiment, \"Word\": word, \"Count\": count})\n",
        "\n",
        "df_top_words = pd.DataFrame(rows)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_top_words, x='Word', y='Count', hue='Sentiment')\n",
        "plt.title(\"Top Words per Sentiment\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18cb6446",
      "metadata": {
        "id": "18cb6446"
      },
      "source": [
        "**Bigram and Trigram Analysis**\n",
        "\n",
        "To uncover common patterns and expressions beyond individual words, bigram and trigram analysis was performed. This helps identify meaningful word combinations that frequently appear together in the tweets, such as \"online shopping\" or \"stay home\". These multi-word phrases often carry more semantic weight than isolated words and can reveal recurring themes, public concerns, and behavior patterns during the COVID-19 pandemic.\n",
        "\n",
        "This analysis complements unigram (single-word) frequency by capturing context and phrase-level insights in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18aeb6d",
      "metadata": {
        "id": "f18aeb6d"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning to all text entries\n",
        "df_train['cleaned_tweets'] = df_train['OriginalTweet'].apply(lambda x: ' '.join(tokenize_and_clean_filtered(x)))\n",
        "\n",
        "# Combine all text into a single corpus\n",
        "corpus = df_train['cleaned_tweets'].dropna().tolist()\n",
        "\n",
        "# Function to plot top n-grams\n",
        "def plot_ngrams(corpus, ngram_range=(2, 2), top_n=20, title='Top N-grams'):\n",
        "    vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    sum_words = X.sum(axis=0)\n",
        "\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    # Create a DataFrame for visualization\n",
        "    ngram_df = pd.DataFrame(words_freq, columns=['ngram', 'count'])\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.barplot(x='count', y='ngram', data=ngram_df, color='skyblue')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('N-gram')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot top bigrams + trigrams\n",
        "plot_ngrams(corpus, ngram_range=(2, 2), title='Top Cleaned Bigrams')\n",
        "plot_ngrams(corpus, ngram_range=(3, 3), title='Top Cleaned Trigrams')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c890184",
      "metadata": {
        "id": "9c890184"
      },
      "source": [
        "**Emoji Analysis**\n",
        "\n",
        "Emojis often convey strong emotional signals and can complement or even replace words in expressing sentiment.<br>\n",
        "However, due to encoding issues, most emojis were corrupted and appeared as symbols like © and ®. Attempts to fix this using different encodings failed. <br>\n",
        "Therefore, this analysis couldn't be completed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064e48cc",
      "metadata": {
        "id": "064e48cc"
      },
      "outputs": [],
      "source": [
        "# import emoji\n",
        "\n",
        "# def extract_emojis(text):\n",
        "#     return [char for char in text if char in emoji.EMOJI_DATA]\n",
        "\n",
        "# # Apply to data\n",
        "# df_train['emojis'] = df_train['OriginalTweet'].apply(extract_emojis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c514ea3",
      "metadata": {
        "id": "5c514ea3"
      },
      "outputs": [],
      "source": [
        "# # Create separate counters\n",
        "# emoji_counters = {\n",
        "#     'Extremely Positive': Counter(),\n",
        "#     'Positive': Counter(),\n",
        "#     'Neutral': Counter(),\n",
        "#     'Negative': Counter(),\n",
        "#     'Extremely Negative': Counter()\n",
        "# }\n",
        "\n",
        "# # Count emojis for each sentiment\n",
        "# for _, row in df_train.iterrows():\n",
        "#     sentiment = row['Sentiment']\n",
        "#     emojis = row['emojis']\n",
        "#     emoji_counters[sentiment].update(emojis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de2a203d",
      "metadata": {
        "id": "de2a203d"
      },
      "outputs": [],
      "source": [
        "# # Plot top 10 emojis per sentiment\n",
        "# for sentiment, counter in emoji_counters.items():\n",
        "#     top_emojis = counter.most_common(10)\n",
        "#     emojis, counts = zip(*top_emojis)\n",
        "\n",
        "#     plt.figure(figsize=(8, 4))\n",
        "#     plt.bar(emojis, counts, color='skyblue')\n",
        "#     plt.title(f\"Top Emojis in {sentiment.capitalize()} Tweets\")\n",
        "#     plt.xlabel(\"Emoji\")\n",
        "#     plt.ylabel(\"Count\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fe95fd",
      "metadata": {
        "id": "f4fe95fd"
      },
      "source": [
        "**Hashtag Analysis**\n",
        "\n",
        "Hashtags provide valuable insight into the main topics users focus on in their tweets. By analyzing hashtags per sentiment, we aim to understand which topics are associated with different emotional tones during the COVID-19 pandemic.\n",
        "\n",
        "The most common hashtags across different sentiment categories show hashtags like `#coronavirus`and `#covid19` variations were the most frequently used as expected, and appeared in tweets of all sentiment types.\n",
        "\n",
        "Some hashtags such as `#toiletpaper` and `#panicbuying` showed stronger associations with specific sentiments, indicating public emotional responses to particular aspects of the crisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "384a7b52",
      "metadata": {
        "id": "384a7b52"
      },
      "outputs": [],
      "source": [
        "# Extract hashtags from each tweet\n",
        "def extract_hashtags(text):\n",
        "    return re.findall(r\"#\\w+\", str(text).lower())\n",
        "\n",
        "# Apply on tweets\n",
        "df_train['hashtags'] = df_train['OriginalTweet'].apply(extract_hashtags)\n",
        "\n",
        "# Flatten all hashtags into a single list\n",
        "all_hashtags = [hashtag for hashtags in df_train['hashtags'] for hashtag in hashtags]\n",
        "\n",
        "# Count most common hashtags\n",
        "hashtag_counts = Counter(all_hashtags).most_common(20)\n",
        "\n",
        "# Convert to DataFrame for plotting\n",
        "hashtag_df = pd.DataFrame(hashtag_counts, columns=[\"hashtag\", \"count\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "hashtag_df.sort_values(\"count\", ascending=True).plot.barh(x=\"hashtag\", y=\"count\", legend=False)\n",
        "plt.title(\"Top 20 Most Common Hashtags\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Hashtag\")\n",
        "plt.grid(axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31d0269",
      "metadata": {
        "id": "e31d0269"
      },
      "outputs": [],
      "source": [
        "# Explode hashtags column to have one hashtag per row\n",
        "exploded = df_train.explode('hashtags')\n",
        "\n",
        "# Group by sentiment and hashtag\n",
        "sentiment_hashtags = exploded.groupby(['Sentiment', 'hashtags']).size().reset_index(name='count')\n",
        "\n",
        "# Filter to top 10 for each sentiment\n",
        "top_hashtags_per_sentiment = sentiment_hashtags.groupby('Sentiment').apply(\n",
        "    lambda x: x.sort_values('count', ascending=False).head(10)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# Plot\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=top_hashtags_per_sentiment, x='count', y='hashtags', hue='Sentiment')\n",
        "plt.title('Top Hashtags per Sentiment')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}