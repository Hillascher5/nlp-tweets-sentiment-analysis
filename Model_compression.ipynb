{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPn2pqoGbj4LKcUKiCqicDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hillascher5/nlp-tweets-sentiment-analysis/blob/main/Model_compression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Compression**"
      ],
      "metadata": {
        "id": "6IFIHeXpuGuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Needed for Google Colab\n",
        "# !pip install --quiet evaluate transformers optuna datasets nltk scikit-learn\n",
        "# !pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "FVCaeHw8-1sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from contextlib import nullcontext\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import Trainer, AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, TrainingArguments, DataCollatorWithPadding\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "from datasets import Dataset\n",
        "from torch.optim import AdamW\n",
        "from torch.quantization import quantize_dynamic\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import prune\n",
        "\n",
        "import time\n",
        "import json\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import wandb\n",
        "wandb.login()\n",
        "# API key - 0cbd7fe3cffd71df993b30edb4fa0db94f114413 - uni\n",
        "# API key - 65fb8494261cc49f8d09e6c57ef80bcad6a653b9 - pers\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "torch.backends.quantized.engine = \"fbgemm\""
      ],
      "metadata": {
        "id": "yJiBeyYBxJWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed_all(42)"
      ],
      "metadata": {
        "id": "F2d7JbFSx2lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "is_preprocessed = 'minimal_preprocess'\n",
        "num_train_samples = 5000"
      ],
      "metadata": {
        "id": "sB9UYsuk2xs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_train.csv', encoding='latin1')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_test.csv', encoding='latin1')"
      ],
      "metadata": {
        "id": "BIkHYOqj1B2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge and shuffle for better stratified splits\n",
        "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
        "df_full = df_full.sample(frac=1.0, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "px06AqYH1ClE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal pre-processing\n",
        "def light_preprocess(text):\n",
        "    return text.strip()                             # Remove unnecessary spaces\n",
        "\n",
        "df_full[\"clean_text\"] = df_full[\"OriginalTweet\"].apply(light_preprocess)"
      ],
      "metadata": {
        "id": "fMH9a46LDfh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sentiments to unique numeric IDs\n",
        "unique_labels = sorted(df_full[\"Sentiment\"].unique())\n",
        "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "df_full[\"label\"] = df_full[\"Sentiment\"].map(label2id)"
      ],
      "metadata": {
        "id": "O1j7Z3KxDfsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified split: 70% train, 15% val, 15% test\n",
        "train_val_df, test_df = train_test_split(df_full, test_size=0.15, stratify=df_full[\"label\"], random_state=42)\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.1765, stratify=train_val_df[\"label\"], random_state=42)\n",
        "\n",
        "# Confirm sizes\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n",
        "print(\"Test size:\", len(test_df))"
      ],
      "metadata": {
        "id": "oaPOmB_DG3RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_subset_df, _ = train_test_split(\n",
        "    train_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=num_train_samples,\n",
        "    stratify=train_df[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_subset_df, _ = train_test_split(\n",
        "    val_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=500,\n",
        "    stratify=val_df[\"label\"],\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "JB6PqcORkGem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSiij_RnuCN0"
      },
      "outputs": [],
      "source": [
        "# Load HF Trainer models\n",
        "bert_trainer_dir = f\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer/bert_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "roberta_trainer_dir = f\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer/roberta_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "deberta_trainer_dir = f\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer/deberta_best_model_stratify_maxl_128_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_trainer_dir).to(device)\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_trainer_dir)\n",
        "print(\"Original bert model (HF) size:\", sum(p.numel() for p in bert_model.parameters()))\n",
        "\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_trainer_dir).to(device)\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_trainer_dir)\n",
        "print(\"Original roberta model (HF) size:\", sum(p.numel() for p in roberta_model.parameters()))\n",
        "\n",
        "deberta_model = AutoModelForSequenceClassification.from_pretrained(deberta_trainer_dir).to(device)\n",
        "deberta_tokenizer = AutoTokenizer.from_pretrained(deberta_trainer_dir)\n",
        "print(\"Original deberta model (HF) size:\", sum(p.numel() for p in deberta_model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to Hugging Face Dataset\n",
        "hf_train = Dataset.from_pandas(train_df[[\"clean_text\", \"label\"]])\n",
        "hf_val = Dataset.from_pandas(val_df[[\"clean_text\", \"label\"]])\n",
        "hf_test = Dataset.from_pandas(test_df[[\"clean_text\", \"label\"]])"
      ],
      "metadata": {
        "id": "sUW_539sHv_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1_macro\": f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"],\n",
        "        \"precision_macro\": precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"],\n",
        "        \"recall_macro\": recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "LHR0kJ0uIOT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison between Compression Methods**"
      ],
      "metadata": {
        "id": "lxaEV5I15xmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for Compression Evaluation\n",
        "\n",
        "The following functions were used to evaluate and compare the compressed versions of the used models (BERT, RoBERTa, and DeBERTa). They support all the experiments conducted on quantization, pruning and knowledge distillation.  \n",
        "\n",
        "Specifically, the functions provide:\n",
        "\n",
        "- **Model size** – measures checkpoint disk usage to compare storage savings.  \n",
        "- **Dataset prep** – tokenizes tweets and labels into tensors for evaluation.  \n",
        "- **Accuracy & F1** – evaluates predictive performance on GPU/CPU.  \n",
        "- **Latency** – benchmarks inference speed (sec/1000 examples, throughput).  \n",
        "  \n",
        "\n",
        "Together, these functions formed the backbone of the experimental pipeline, ensuring that baseline and compressed models were evaluated under the same conditions for fairness. This enabled a consistent comparison across techniques and models.\n"
      ],
      "metadata": {
        "id": "nQaDQV9itgj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def model_disk_size(path_or_file):\n",
        "    if os.path.isdir(path_or_file):\n",
        "        total = 0\n",
        "        for root, _, files in os.walk(path_or_file):\n",
        "            for f in files:\n",
        "                total += os.path.getsize(os.path.join(root, f))\n",
        "        return total / (1024**2)  # MB\n",
        "    return os.path.getsize(path_or_file) / (1024**2)\n",
        "\n",
        "\n",
        "def texts_to_dataset(tokenizer, texts, labels=None, max_length=256):\n",
        "    enc = tokenizer(list(texts), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    if labels is None:\n",
        "        return TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
        "    return TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"], torch.tensor(labels, dtype=torch.long))\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_accuracy_f1(model, tokenizer, texts, labels, batch_size=32, max_length=256, amp=False):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    ds = texts_to_dataset(tokenizer, texts, labels, max_length)\n",
        "    dl = DataLoader(ds, batch_size=batch_size)\n",
        "    preds, gts = [], []\n",
        "    ctx = torch.cuda.amp.autocast() if (amp and device.type == \"cuda\") else nullcontext()\n",
        "    for batch in dl:\n",
        "        if len(batch) == 3:\n",
        "            input_ids, attn_mask, y = batch\n",
        "        else:\n",
        "            input_ids, attn_mask = batch; y = None\n",
        "        input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "        with ctx:\n",
        "            logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "        preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "        if y is not None:\n",
        "            gts.extend(y.numpy())\n",
        "    acc = accuracy_score(gts, preds)\n",
        "    f1m = f1_score(gts, preds, average=\"macro\")\n",
        "    return acc, f1m\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_accuracy_f1_cpu(model, tokenizer, texts, labels, batch_size=32, max_length=256):\n",
        "    model.cpu().eval()                 # <-- force CPU\n",
        "    # IMPORTANT: no autocast / AMP here\n",
        "    from torch.utils.data import TensorDataset, DataLoader\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import accuracy_score, f1_score\n",
        "    enc = tokenizer(list(texts), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    ds = TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"], torch.tensor(labels, dtype=torch.long))\n",
        "    dl = DataLoader(ds, batch_size=batch_size)\n",
        "    preds, gts = [], []\n",
        "    for input_ids, attn_mask, y in dl:\n",
        "        logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "        preds.extend(logits.argmax(1).cpu().numpy())\n",
        "        gts.extend(y.cpu().numpy())\n",
        "    acc = accuracy_score(gts, preds)\n",
        "    f1m = f1_score(gts, preds, average=\"macro\")\n",
        "    return acc, f1m\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_latency(model, tokenizer, texts, batch_size=32, warmup=2, runs=5, max_length=256, amp=False):\n",
        "    \"\"\"\n",
        "    Returns (sec_per_1000_examples, throughput_examples_per_sec)\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    ds = texts_to_dataset(tokenizer, texts, None, max_length)\n",
        "    dl = DataLoader(ds, batch_size=batch_size)\n",
        "    ctx = torch.cuda.amp.autocast() if (amp and device.type == \"cuda\") else nullcontext()\n",
        "\n",
        "    # Warmup\n",
        "    it = iter(dl)\n",
        "    for _ in range(warmup):\n",
        "        try:\n",
        "            input_ids, attn_mask = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(dl); input_ids, attn_mask = next(it)\n",
        "        input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "        with ctx:\n",
        "            _ = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Timed passes\n",
        "    totals = []\n",
        "    for _ in range(runs):\n",
        "        seen = 0\n",
        "        t0 = time.perf_counter()\n",
        "        for input_ids, attn_mask in dl:\n",
        "            input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "            with ctx:\n",
        "                _ = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "            seen += input_ids.size(0)\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        totals.append((time.perf_counter() - t0, seen))\n",
        "\n",
        "    mean_time = np.mean([t for t, _ in totals])\n",
        "    mean_seen = np.mean([s for _, s in totals])\n",
        "    sec_per_1000 = mean_time * (1000.0 / mean_seen)\n",
        "    throughput = mean_seen / mean_time\n",
        "    return sec_per_1000, throughput\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_latency_cpu(model, tokenizer, texts, batch_size=32, runs=3, max_length=256):\n",
        "    model.cpu().eval()\n",
        "    enc = tokenizer(list(texts), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "    ds  = TensorDataset(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
        "    dl  = DataLoader(ds, batch_size=batch_size)\n",
        "\n",
        "    # Warmup (1 pass)\n",
        "    for input_ids, attn_mask in dl:\n",
        "        _ = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "        break\n",
        "\n",
        "    # Timed passes\n",
        "    totals = []\n",
        "    for _ in range(runs):\n",
        "        seen = 0\n",
        "        t0 = time.perf_counter()\n",
        "        for input_ids, attn_mask in dl:\n",
        "            _ = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "            seen += input_ids.size(0)\n",
        "        totals.append((time.perf_counter() - t0, seen))\n",
        "\n",
        "    mean_time = np.mean([t for t, _ in totals])\n",
        "    mean_seen = np.mean([s for _, s in totals])\n",
        "    sec_per_1000 = mean_time * (1000.0 / mean_seen)\n",
        "    throughput   = mean_seen / mean_time\n",
        "    return sec_per_1000, throughput"
      ],
      "metadata": {
        "id": "37_yIIWMBDgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following loads baseline FP32 models and applies **dynamic quantization** either to all linear layers (BERT/RoBERTa) or only the classification head (DeBERTa-v3), enabling lightweight models for faster inference while preserving accuracy.  \n"
      ],
      "metadata": {
        "id": "BbKur-H1v3Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fp32_hf(model_dir_or_id: str, num_labels: int, map_location=\"cpu\"):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir_or_id, num_labels=num_labels)\n",
        "    tok   = AutoTokenizer.from_pretrained(model_dir_or_id)\n",
        "    model.to(map_location).eval()\n",
        "    return model, tok\n",
        "\n",
        "def quantize_full_linear(model_cpu):\n",
        "    # quantize ALL nn.Linear layers (good for BERT/RoBERTa)\n",
        "    return quantize_dynamic(model_cpu, {nn.Linear}, dtype=torch.qint8).eval()\n",
        "\n",
        "def quantize_head_only(model_cpu):\n",
        "    # quantize ONLY the classification head (safer for DeBERTa-v3)\n",
        "    q = copy.deepcopy(model_cpu).eval()\n",
        "    if hasattr(q, \"classifier\") and isinstance(q.classifier, nn.Module):\n",
        "        q.classifier = quantize_dynamic(q.classifier, {nn.Linear}, dtype=torch.qint8).eval()\n",
        "    return q\n",
        "\n",
        "def load_int8_dynamic_from_baseline(baseline_dir: str, num_labels: int, strategy: str):\n",
        "    # Load FP32 on CPU, then quantize on CPU\n",
        "    model_cpu, tok = load_fp32_hf(baseline_dir, num_labels=num_labels, map_location=\"cpu\")\n",
        "    if strategy == \"full\":\n",
        "        model_q = quantize_full_linear(model_cpu)\n",
        "    elif strategy == \"head\":\n",
        "        model_q = quantize_head_only(model_cpu)\n",
        "    else:\n",
        "        raise ValueError(\"strategy must be 'full' or 'head'\")\n",
        "    return model_q, tok"
      ],
      "metadata": {
        "id": "0khxKDrdWtVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function saves a given model variant (quantized, pruned, or distilled) together with its tokenizer, either in Hugging Face format or as raw PyTorch weights, ensuring reproducibility and easy reloading for further experiments.  \n"
      ],
      "metadata": {
        "id": "vWrVxl0twu2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model_variant(\n",
        "    model_name: str,\n",
        "    variant: str,                 # \"quantized\", \"pruned\", \"distilled\"\n",
        "    model,\n",
        "    tokenizer,\n",
        "    base_dir: str,\n",
        "    save_type: str = \"hf\",        # \"hf\" | \"torch\"\n",
        "    filename_stub: str = None     # f\"{model_name}_{variant}\"\n",
        "):\n",
        "    folder = f\"{model_name}_{variant}_model\"\n",
        "    save_dir = os.path.join(base_dir, folder)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    stub = filename_stub or f\"{model_name}_{variant}\"\n",
        "\n",
        "    if save_type == \"hf\":\n",
        "        # HF format\n",
        "        model.save_pretrained(save_dir)\n",
        "        tokenizer.save_pretrained(save_dir)\n",
        "        print(f\"Saved {model_name} ({variant}, HF) -> {save_dir}\")\n",
        "\n",
        "    elif save_type == \"torch\":\n",
        "        # CPU is safer for pickled quantized models\n",
        "        model = model.to(\"cpu\")\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, f\"{stub}.pt\"))\n",
        "        tokenizer.save_pretrained(save_dir)  # tokenizer can still be HF\n",
        "        print(f\"Saved {model_name} ({variant}, torch) -> {save_dir}\")\n",
        "    else:\n",
        "        raise ValueError(\"save_type must be one of: 'hf', 'torch'\")\n",
        "\n",
        "    return save_dir"
      ],
      "metadata": {
        "id": "5_lmtohzp7KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_save_dir = \"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer/Compressed_models\"\n",
        "\n",
        "test_texts  = list(test_df[\"clean_text\"])\n",
        "test_labels = test_df[\"label\"].values"
      ],
      "metadata": {
        "id": "yaUl_av1wd0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization"
      ],
      "metadata": {
        "id": "KgWGJpmtBvpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying dynamic quantization to all models (BERT, RoBERTa, DeBERTa) and saving the quantized variants for later evaluation.  \n"
      ],
      "metadata": {
        "id": "Nq9anXFxxN7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_quantization(model):\n",
        "    return quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "bert_quantized = apply_quantization(bert_model)\n",
        "roberta_quantized = apply_quantization(roberta_model)\n",
        "deberta_quantized = apply_quantization(deberta_model)\n",
        "\n",
        "# Quantized model must be on CPU\n",
        "bert_quantized = bert_quantized.to(\"cpu\").eval()\n",
        "roberta_quantized = roberta_quantized.to(\"cpu\").eval()\n",
        "deberta_quantized = deberta_quantized.to(\"cpu\").eval()\n",
        "\n",
        "print(\"BERT quantized size:\", sum(p.numel() for p in bert_quantized.parameters()))\n",
        "print(\"RoBERTa quantized size:\", sum(p.numel() for p in roberta_quantized.parameters()))\n",
        "print(\"DeBERTa quantized size:\", sum(p.numel() for p in deberta_quantized.parameters()))\n",
        "\n",
        "# Save quantized models\n",
        "bert_quant_dir = save_model_variant(\"bert\", \"quantized\", bert_quantized, bert_tokenizer, base_save_dir, save_type=\"torch\")\n",
        "roberta_quant_dir = save_model_variant(\"roberta\", \"quantized\", roberta_quantized, roberta_tokenizer, base_save_dir, save_type=\"torch\")\n",
        "deberta_quant_dir = save_model_variant(\"deberta\", \"quantized\", deberta_quantized, deberta_tokenizer, base_save_dir, save_type=\"torch\")"
      ],
      "metadata": {
        "id": "xY6qwy3yANtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruning"
      ],
      "metadata": {
        "id": "K51l9fuGBz2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applies pruning to the linear layers of each model (removing 40% of weights), then performing short recovery fine-tuning on subsets of the training and validation data to regain accuracy. Pruned models are saved for later evaluation.  \n"
      ],
      "metadata": {
        "id": "yxJf0WKhxq30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pruning(model, amount=0.2):\n",
        "    modules = [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "    parameters_to_prune = [(m, \"weight\") for m in modules]\n",
        "\n",
        "    prune.global_unstructured(\n",
        "        parameters_to_prune,\n",
        "        pruning_method=prune.L1Unstructured,\n",
        "        amount=amount,\n",
        "    )\n",
        "    # Make pruning permanent\n",
        "    for m in modules:\n",
        "        if hasattr(m, \"weight_mask\"):\n",
        "            prune.remove(m, \"weight\")\n",
        "    return model\n",
        "\n",
        "# Fine-tuning after pruning to restore lost accuracy caused due to the removed weights\n",
        "def recover_finetune(model, tokenizer, train_texts, train_labels, val_texts, val_labels, max_length=256, epochs=1, lr=1e-5, bs=32):\n",
        "    model.train().to(device)\n",
        "    train_ds = texts_to_dataset(tokenizer, train_texts, train_labels, max_length)\n",
        "    val_ds   = texts_to_dataset(tokenizer, val_texts,   val_labels,   max_length)\n",
        "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=bs)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    for _ in range(epochs):\n",
        "        for input_ids, attn_mask, y in train_dl:\n",
        "            input_ids, attn_mask, y = input_ids.to(device), attn_mask.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            loss = model(input_ids=input_ids, attention_mask=attn_mask, labels=y).loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "bert_pruned = apply_pruning(bert_model, amount=0.4)\n",
        "roberta_pruned = apply_pruning(roberta_model, amount=0.4)\n",
        "deberta_pruned = apply_pruning(deberta_model, amount=0.4)\n",
        "print(\"Pruned bert model non-zero parameters:\", sum((p != 0).sum().item() for p in bert_pruned.parameters()))\n",
        "print(\"Pruned roberta model non-zero parameters:\", sum((p != 0).sum().item() for p in roberta_pruned.parameters()))\n",
        "print(\"Pruned deberta model non-zero parameters:\", sum((p != 0).sum().item() for p in deberta_pruned.parameters()))\n",
        "\n",
        "# Recover fine-tune\n",
        "bert_model_recov = recover_finetune(bert_pruned, bert_tokenizer, train_subset_df[\"clean_text\"].to_list(), train_subset_df[\"label\"].to_list(), val_subset_df['clean_text'].to_list(), val_subset_df['label'].to_list(), max_length=256, epochs=2, lr=1e-5, bs=32)\n",
        "roberta_model_recov = recover_finetune(roberta_pruned, roberta_tokenizer, train_subset_df[\"clean_text\"].to_list(), train_subset_df[\"label\"].to_list(), val_subset_df['clean_text'].to_list(), val_subset_df['label'].to_list(), max_length=256, epochs=2, lr=1e-5, bs=32)\n",
        "deberta_model_recov = recover_finetune(deberta_pruned, deberta_tokenizer, train_subset_df[\"clean_text\"].to_list(), train_subset_df[\"label\"].to_list(), val_subset_df['clean_text'].to_list(), val_subset_df['label'].to_list(), max_length=128, epochs=2, lr=1e-5, bs=32)\n",
        "\n",
        "# Save pruned models\n",
        "bert_pruned_dir = save_model_variant(\"bert\", \"pruned\", bert_model_recov, bert_tokenizer, base_save_dir, save_type=\"hf\")\n",
        "roberta_pruned_dir = save_model_variant(\"roberta\", \"pruned\", roberta_model_recov, roberta_tokenizer, base_save_dir, save_type=\"hf\")\n",
        "deberta_pruned_dir = save_model_variant(\"deberta\", \"pruned\", deberta_model_recov, deberta_tokenizer, base_save_dir, save_type=\"hf\")"
      ],
      "metadata": {
        "id": "YvKal3s2BsWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Knowledge Distillation"
      ],
      "metadata": {
        "id": "sy1Y6zZkCy9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining students\n",
        "bert_student_id = \"distilbert-base-uncased\"\n",
        "roberta_student_id = \"distilroberta-base\"\n",
        "deberta_student_id = \"microsoft/deberta-v3-small\"\n",
        "\n",
        "student_map ={\n",
        "    \"bert\": bert_student_id,\n",
        "    \"roberta\": roberta_student_id,\n",
        "    \"deberta\": deberta_student_id,\n",
        "}"
      ],
      "metadata": {
        "id": "n6PlJlhqJuPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing a class DistillationTrainer that combines **cross-entropy loss** with **knowledge distillation loss**. A large teacher model guides a smaller student model by providing softened probability distributions, improving generalization and efficiency. A function that handles tokenization, training, evaluation, and saving the distilled student model.\n"
      ],
      "metadata": {
        "id": "JvA9EuPLzQYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillationTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher_model=None, temperature=4.0, alpha=0.5, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        assert teacher_model is not None, \"teacher_model is required\"\n",
        "        self.teacher = teacher_model.eval()\n",
        "        for p in self.teacher.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.temperature = temperature\n",
        "        self.alpha = alpha\n",
        "        self._teacher_device = None  # where the teacher currently is\n",
        "\n",
        "    def _ensure_teacher_device(self, ref_device):\n",
        "        # Move teacher once if device changed\n",
        "        if self._teacher_device != ref_device:\n",
        "            self.teacher.to(ref_device)\n",
        "            self._teacher_device = ref_device\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\", inputs.get(\"label\", None))\n",
        "        if labels is None:\n",
        "            raise ValueError(\"No labels found in inputs (expected 'labels' or 'label').\")\n",
        "\n",
        "        # Ensure teacher is on the same device as the student/labels\n",
        "        ref_device = labels.device\n",
        "        self._ensure_teacher_device(ref_device)\n",
        "\n",
        "        # student forward\n",
        "        outputs_s = model(**inputs)\n",
        "        logits_s = outputs_s.logits\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_t = self.teacher(**inputs)\n",
        "            logits_t  = outputs_t.logits\n",
        "\n",
        "        # CE\n",
        "        loss_ce = F.cross_entropy(logits_s, labels)\n",
        "\n",
        "        # soft-label KD\n",
        "        T = self.temperature\n",
        "        loss_kd = F.kl_div(\n",
        "            F.log_softmax(logits_s / T, dim=-1),\n",
        "            F.softmax(logits_t / T, dim=-1),\n",
        "            reduction=\"batchmean\",\n",
        "        ) * (T * T)\n",
        "\n",
        "        loss = self.alpha * loss_kd + (1.0 - self.alpha) * loss_ce\n",
        "        return (loss, outputs_s) if return_outputs else loss\n",
        "\n",
        "# Tokenize helper\n",
        "def build_tokenized_splits(train_df, val_df, test_df, tokenizer, max_length=256):\n",
        "    def tok(batch):\n",
        "        return tokenizer(\n",
        "            batch[\"clean_text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "        )\n",
        "\n",
        "    hf_train = Dataset.from_pandas(train_df[[\"clean_text\", \"label\"]])\n",
        "    hf_val   = Dataset.from_pandas(val_df[[\"clean_text\", \"label\"]])\n",
        "    hf_test  = Dataset.from_pandas(test_df[[\"clean_text\", \"label\"]])\n",
        "\n",
        "    t_train = hf_train.map(tok, batched=True)\n",
        "    t_val   = hf_val.map(tok, batched=True)\n",
        "    t_test  = hf_test.map(tok, batched=True)\n",
        "\n",
        "    cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "    t_train.set_format(\"torch\", columns=cols)\n",
        "    t_val.set_format(\"torch\", columns=cols)\n",
        "    t_test.set_format(\"torch\", columns=cols)\n",
        "    return t_train, t_val, t_test\n",
        "\n",
        "# Distillation runner\n",
        "def distill_one(\n",
        "    teacher_id: str,\n",
        "    student_id: str,\n",
        "    train_df,\n",
        "    val_df,\n",
        "    test_df,\n",
        "    num_labels: int,\n",
        "    out_dir: str,\n",
        "    run_name: str,\n",
        "    max_length: int = 256,\n",
        "    learning_rate: float = 5e-5,\n",
        "    batch_size: int = 32,\n",
        "    num_train_epochs: int = 3,\n",
        "    weight_decay: float = 0.01,\n",
        "    temperature: float = 4.0,\n",
        "    alpha: float = 0.5,\n",
        "    use_teacher_tokenizer: bool = True,\n",
        "    seed: int = 42,\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    torch.manual_seed(seed); np.random.seed(seed)\n",
        "\n",
        "    # Tokenizer\n",
        "    tok_id = teacher_id if use_teacher_tokenizer else student_id\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tok_id, use_fast=True)\n",
        "\n",
        "    # Build splits\n",
        "    t_train, t_val, t_test = build_tokenized_splits(train_df, val_df, test_df, tokenizer, max_length=max_length)\n",
        "\n",
        "    # Models\n",
        "    teacher = AutoModelForSequenceClassification.from_pretrained(teacher_id, num_labels=num_labels)\n",
        "    student = AutoModelForSequenceClassification.from_pretrained(student_id, num_labels=num_labels)\n",
        "\n",
        "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    args = TrainingArguments(\n",
        "        output_dir=os.path.join(out_dir, \"student_ckpt\"),\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=weight_decay,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        logging_strategy=\"epoch\",\n",
        "        run_name=run_name,\n",
        "        seed=seed,\n",
        "    )\n",
        "\n",
        "    trainer = DistillationTrainer(\n",
        "        teacher_model=teacher,\n",
        "        temperature=temperature,\n",
        "        alpha=alpha,\n",
        "        model=student,\n",
        "        args=args,\n",
        "        train_dataset=t_train,\n",
        "        eval_dataset=t_val,\n",
        "        data_collator=collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    val_metrics = trainer.evaluate()\n",
        "    print(\"Best (val):\", val_metrics)\n",
        "\n",
        "    # Evaluate on test\n",
        "    test_metrics = trainer.evaluate(eval_dataset=t_test)\n",
        "    print(\"Test:\", test_metrics)\n",
        "\n",
        "    # Save distilled student\n",
        "    save_dir = os.path.join(out_dir, \"student_distilled\")\n",
        "    trainer.save_model(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"Saved student to {save_dir}\")\n",
        "\n",
        "    return {\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"save_dir\": save_dir,\n",
        "        \"student_id\": student_id,\n",
        "        \"teacher_id\": teacher_id,\n",
        "        \"max_length\": max_length,\n",
        "    }"
      ],
      "metadata": {
        "id": "VnITdVKNulpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distills each base model (teacher) into a smaller student version.    \n",
        "Each run trains the student with guidance from its teacher, evaluates on validation and test sets, logs results to Weights & Biases, and saves the distilled models.\n"
      ],
      "metadata": {
        "id": "lJ8RVEP-0TPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = len(label2id)\n",
        "n_epochs = 3\n",
        "alpha = 0.5\n",
        "T = 4.0\n",
        "max_l_bert_roberta = 256\n",
        "max_l_deberta = 128\n",
        "lr = 5e-5\n",
        "bs = 32\n",
        "\n",
        "# BERT\n",
        "res_bert = distill_one(\n",
        "    teacher_id=\"bert-base-uncased\",\n",
        "    student_id=student_map[\"bert\"],\n",
        "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "    num_labels=num_labels,\n",
        "    out_dir=\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer/Compressed_models/bert_distill\",\n",
        "    run_name=f\"bert_kd_T{T}_a{alpha}_e{n_epochs}\",\n",
        "    max_length=max_l_bert_roberta, learning_rate=lr, batch_size=bs, num_train_epochs=n_epochs,\n",
        "    temperature=T, alpha=alpha)\n",
        "wandb.finish()\n",
        "\n",
        "# RoBERTa\n",
        "res_roberta = distill_one(\n",
        "    teacher_id=\"roberta-base\",\n",
        "    student_id=student_map[\"roberta\"],\n",
        "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "    num_labels=num_labels,\n",
        "    out_dir=\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer/Compressed_models/roberta_distill\",\n",
        "    run_name=f\"roberta_kd_T{T}_a{alpha}_e{n_epochs}\",\n",
        "    max_length=max_l_bert_roberta, learning_rate=lr, batch_size=bs, num_train_epochs=n_epochs,\n",
        "    temperature=T, alpha=alpha)\n",
        "wandb.finish()\n",
        "\n",
        "# DeBERTa\n",
        "res_deberta = distill_one(\n",
        "    teacher_id=\"microsoft/deberta-v3-base\",\n",
        "    student_id=student_map[\"deberta\"],\n",
        "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "    num_labels=num_labels,\n",
        "    out_dir=\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer/Compressed_models/deberta_distill\",\n",
        "    run_name=f\"deberta_kd_T{T}_a{alpha}_e{n_epochs}\",\n",
        "    max_length=max_l_deberta,\n",
        "    learning_rate=lr, batch_size=bs, num_train_epochs=n_epochs,\n",
        "    temperature=T, alpha=alpha)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "sMmKe3SayimT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating a single model variant on accuracy, macro-F1, size, and latency.  \n",
        "If the variant is INT8 Dynamic, it loads a quantized CPU-only model; otherwise it loads a Hugging Face model.  \n",
        "Returning performance and efficiency metrics for easy comparison across variants.\n"
      ],
      "metadata": {
        "id": "IzD7NSYf1LAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hf(path_or_dir, num_labels):\n",
        "    tok = AutoTokenizer.from_pretrained(path_or_dir)\n",
        "    mdl = AutoModelForSequenceClassification.from_pretrained(path_or_dir, num_labels=num_labels)\n",
        "    return mdl, tok\n",
        "\n",
        "\n",
        "def eval_one_row(model_name, variant_name, variant_cfg, max_length, texts, labels,\n",
        "                 latency_sample=1500, batch_size=32, amp=True, num_labels=5):\n",
        "    if variant_name == \"INT8 Dynamic\":\n",
        "        strategy = variant_cfg.get(\"strategy\", \"full\")\n",
        "        model, tokenizer = load_int8_dynamic_from_baseline(\n",
        "            variant_cfg[\"quantize_from\"], num_labels=num_labels, strategy=strategy\n",
        "        )\n",
        "        size_mb = model_disk_size(variant_cfg[\"quantize_from\"])\n",
        "        # CPU-only latency and eval\n",
        "        lat, _ = measure_latency_cpu(model, tokenizer, texts[:latency_sample],\n",
        "                                     batch_size=batch_size, max_length=max_length)\n",
        "        acc, f1m = eval_accuracy_f1_cpu(model, tokenizer, texts, labels,\n",
        "                                        batch_size=batch_size, max_length=max_length)\n",
        "        return {\n",
        "            \"Model\": model_name,\n",
        "            \"Variant\": variant_name + (f\" ({strategy})\" if model_name==\"DeBERTa\" else \"\"),\n",
        "            \"MaxLen\": max_length,\n",
        "            \"Size_MB\": round(size_mb, 2),\n",
        "            \"Sec_per_1000\": round(lat, 3),\n",
        "            \"Accuracy\": round(acc, 4),\n",
        "            \"MacroF1\": round(f1m, 4),\n",
        "        }\n",
        "\n",
        "\n",
        "    model, tokenizer = load_hf(variant_cfg[\"path\"], num_labels=num_labels)\n",
        "    size_mb = model_disk_size(variant_cfg[\"path\"])\n",
        "    model = model.to(device).eval()\n",
        "    lat, _ = measure_latency(model, tokenizer, texts[:latency_sample],\n",
        "                             batch_size=batch_size, runs=3, max_length=max_length, amp=amp)\n",
        "    acc, f1m = eval_accuracy_f1(model, tokenizer, texts, labels,\n",
        "                                batch_size=batch_size, max_length=max_length, amp=amp)\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Variant\": variant_name,\n",
        "        \"MaxLen\": max_length,\n",
        "        \"Size_MB\": round(size_mb, 2),\n",
        "        \"Sec_per_1000\": round(lat, 3),\n",
        "        \"Accuracy\": round(acc, 4),\n",
        "        \"MacroF1\": round(f1m, 4),\n",
        "    }"
      ],
      "metadata": {
        "id": "uT8MvocZO6wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining configuration for each model family (BERT, RoBERTa, DeBERTa), including their baseline, quantized, pruned, and distilled variants.  \n",
        "Each variant is evaluated on accuracy, F1, latency, and size, and the results are collected into a comparison DataFrame for analysis.\n"
      ],
      "metadata": {
        "id": "6GkoDohm1wIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = \"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/HF_Trainer\"\n",
        "COMP = f\"{BASE}/Compressed_models\"\n",
        "\n",
        "cfg = {\n",
        "    \"BERT\": {\n",
        "        \"maxlen\": 256,\n",
        "        \"baseline\":  {\"path\": f\"{BASE}/bert_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"},\n",
        "        \"quantized\": {\"quantize_from\": f\"{BASE}/bert_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\",\n",
        "                      \"strategy\": \"full\"},\n",
        "        \"pruned\":    {\"path\": f\"{COMP}/bert_pruned_model\"},\n",
        "        \"distilled\": {\"path\": f\"{COMP}/bert_distill/student_distilled\"},\n",
        "    },\n",
        "    \"RoBERTa\": {\n",
        "        \"maxlen\": 256,\n",
        "        \"baseline\":  {\"path\": f\"{BASE}/roberta_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"},\n",
        "        \"quantized\": {\"quantize_from\": f\"{BASE}/roberta_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\",\n",
        "                      \"strategy\": \"full\"},\n",
        "        \"pruned\":    {\"path\": f\"{COMP}/roberta_pruned_model\"},\n",
        "        \"distilled\": {\"path\": f\"{COMP}/roberta_distill/student_distilled\"},\n",
        "    },\n",
        "    \"DeBERTa\": {\n",
        "        \"maxlen\": 128,\n",
        "        \"baseline\":  {\"path\": f\"{BASE}/deberta_best_model_stratify_maxl_128_{is_preprocessed}_{num_train_samples}_samples_optuna\"},\n",
        "        \"quantized\": {\"quantize_from\": f\"{BASE}/deberta_best_model_stratify_maxl_128_{is_preprocessed}_{num_train_samples}_samples_optuna\",\n",
        "                      \"strategy\": \"head\"},\n",
        "        \"pruned\":    {\"path\": f\"{COMP}/deberta_pruned_model\"},\n",
        "        \"distilled\": {\"path\": f\"{COMP}/deberta_distill/student_distilled\"},\n",
        "    },\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for name, fam in cfg.items():\n",
        "    maxlen = fam[\"maxlen\"]\n",
        "    rows.append(eval_one_row(name, \"Baseline\",     fam[\"baseline\"],  maxlen, test_texts, test_labels))\n",
        "    rows.append(eval_one_row(name, \"INT8 Dynamic\", fam[\"quantized\"], maxlen, test_texts, test_labels))\n",
        "    rows.append(eval_one_row(name, \"Pruned\",       fam[\"pruned\"],    maxlen, test_texts, test_labels))\n",
        "    rows.append(eval_one_row(name, \"Distilled\",    fam[\"distilled\"], maxlen, test_texts, test_labels))\n",
        "\n",
        "comp_df = pd.DataFrame(rows).sort_values([\"Model\",\"Variant\"]).reset_index(drop=True)\n",
        "comp_df"
      ],
      "metadata": {
        "id": "PyP4UCLOUm7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iaLxEtxEHoq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Javascript\n",
        "\n",
        "# def disconnect_runtime():\n",
        "#     display(Javascript('google.colab.kernel.disconnect()'))\n",
        "\n",
        "# disconnect_runtime()"
      ],
      "metadata": {
        "id": "y_J3HAwTHLBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OOjVygfvHNlJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}