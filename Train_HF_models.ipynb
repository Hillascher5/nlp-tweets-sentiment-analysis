{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5728f352",
   "metadata": {},
   "source": [
    "## **NLP - Text Classification Project**\n",
    "Group H - August 2025\n",
    "\n",
    "Classification of tweets from Twitter that have been manually tagged for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Needed for Google Colab\n",
    "# !pip install --quiet evaluate transformers optuna datasets nltk scikit-learn\n",
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from optuna.pruners import MedianPruner\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import glob\n",
    "import nltk\n",
    "import evaluate\n",
    "import transformers\n",
    "import torch\n",
    "import optuna\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"tweet-sentiment-classification\"\n",
    "os.environ[\"WANDB_INIT_TIMEOUT\"] = \"180\"\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_train.csv', encoding='latin1')\n",
    "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_test.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and shuffle for better stratified splits\n",
    "df_full = pd.concat([df_train, df_test_original], ignore_index=True)\n",
    "df_full = df_full.sample(frac=1.0, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f623d",
   "metadata": {},
   "source": [
    "### Pre-processing the Data\n",
    "\n",
    "The tweets were cleaned by lowercasing (to reduce redundancy), removing stopwords, punctuation, numbers, short words, and applying lemmatization to reduce words to their base form (e.g. running → run). <br>This helps reduce noise and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abd4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try without pre-processing\n",
    "# df_train[\"clean_text\"] = df_train[\"OriginalTweet\"]\n",
    "# df_val[\"clean_text\"] = df_val[\"OriginalTweet\"]\n",
    "df_full[\"clean_text\"] = df_full[\"OriginalTweet\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e48007",
   "metadata": {},
   "source": [
    "## Fine-Tuning Pretrained Language Models\n",
    "\n",
    "Apply NLP techniques using transfer learning on our tweets dataset. Specifically, fine-tuning two pretrained transformer-based models from the Hugging Face library — BERT and RoBERTa — on our sentiment classification task. These models will be trained using both standard PyTorch and the Hugging Face API. Model performance will be monitored and tuned using hyperparameter optimization (Optuna) and experiment tracking (Weights & Biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f809a00",
   "metadata": {},
   "source": [
    "**Load Pretrained Models**\n",
    "\n",
    "Initialize tokenizers and models for BERT and RoBERTa, both widely used transformer architectures for text classification. The classification head is configured based on the number of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2655478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose pretrained models\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "roberta_model_name = \"roberta-base\"\n",
    "\n",
    "sentiment_labels = df_train['Sentiment'].unique()\n",
    "n_labels = len(sentiment_labels)\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=n_labels)\n",
    "\n",
    "# Load RoBERTa tokenizer and model\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name, num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95eef6",
   "metadata": {},
   "source": [
    "**Encode Sentiment Labels**\n",
    "\n",
    "Map each unique sentiment label to a numeric ID for model compatibility, and apply this mapping to both training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping sentiments to unique numeric IDs\n",
    "# unique_labels = sorted(df_train[\"Sentiment\"].unique())\n",
    "\n",
    "# label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "# id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# df_train[\"label\"] = df_train[\"Sentiment\"].map(label2id)\n",
    "# df_val[\"label\"] = df_val[\"Sentiment\"].map(label2id)\n",
    "unique_labels = sorted(df_full[\"Sentiment\"].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "df_full[\"label\"] = df_full[\"Sentiment\"].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split: 70% train, 15% val, 15% test\n",
    "train_val_df, test_df = train_test_split(df_full, test_size=0.15, stratify=df_full[\"label\"], random_state=42)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.1765, stratify=train_val_df[\"label\"], random_state=42) \n",
    "\n",
    "# Confirm sizes\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d2aae",
   "metadata": {},
   "source": [
    "**Use Small Subsets for Quick Evaluation**\n",
    "\n",
    "Select shuffled samples from each training and validation dataset for both BERT and RoBERTa. This allows faster experimentation during model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_df, _ = train_test_split(\n",
    "    train_df[[\"clean_text\", \"label\"]],\n",
    "    train_size=2000,\n",
    "    stratify=train_df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_subset_df, _ = train_test_split(\n",
    "    val_df[[\"clean_text\", \"label\"]],\n",
    "    train_size=500,\n",
    "    stratify=val_df[\"label\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6baec2",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "Define tokenization functions for BERT and RoBERTa to preprocess text with truncation and fixed padding.\n",
    "\n",
    "Transform training and validation DataFrames into Dataset objects compatible with Hugging Face workflows.\n",
    "\n",
    "Apply tokenization to training and validation datasets using each model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_function_bert(examples):\n",
    "    return bert_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=64)\n",
    "\n",
    "def tokenize_function_roberta(examples):\n",
    "    return roberta_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab47dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset\n",
    "hf_dataset_train = Dataset.from_pandas(df_train[[\"clean_text\", \"label\"]])\n",
    "hf_dataset_val = Dataset.from_pandas(df_val[[\"clean_text\", \"label\"]])\n",
    "\n",
    "hf_subset_train = Dataset.from_pandas(train_subset_df)\n",
    "hf_subset_val = Dataset.from_pandas(val_subset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize subsets\n",
    "# Tokenize for BERT\n",
    "tokenized_bert_train_sub = hf_subset_train.map(tokenize_function_bert, batched=True)\n",
    "tokenized_bert_train_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "tokenized_bert_val_sub = hf_subset_val.map(tokenize_function_bert, batched=True)\n",
    "tokenized_bert_val_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Tokenize for RoBERTa\n",
    "tokenized_roberta_train_sub = hf_subset_train.map(tokenize_function_roberta, batched=True)\n",
    "tokenized_roberta_train_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "tokenized_roberta_val_sub = hf_subset_val.map(tokenize_function_roberta, batched=True)\n",
    "tokenized_roberta_val_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efa62d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize full dataset\n",
    "# Tokenize for BERT\n",
    "tokenized_bert_train = hf_dataset_train.map(tokenize_function_bert, batched=True)\n",
    "tokenized_bert_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "tokenized_bert_val = hf_dataset_val.map(tokenize_function_bert, batched=True)\n",
    "tokenized_bert_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Tokenize for RoBERTa\n",
    "tokenized_roberta_train = hf_dataset_train.map(tokenize_function_roberta, batched=True)\n",
    "tokenized_roberta_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "tokenized_roberta_val = hf_dataset_val.map(tokenize_function_roberta, batched=True)\n",
    "tokenized_roberta_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf48fe",
   "metadata": {},
   "source": [
    "**Hyperparameters Tuning with Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(model_checkpoint, trial, run_prefix, train_dataset, val_dataset):\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    num_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
    "    n_samples = len(train_dataset)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=5)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    run_name = f\"{run_prefix}-ep{num_epochs}-lr{learning_rate}-bs{batch_size}-samples{n_samples}-run{int(time.time())}-no_preprocess\"\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results/{run_prefix}/{run_name}\",\n",
    "        disable_tqdm=True,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=1,\n",
    "        logging_strategy=\"epoch\",\n",
    "        logging_dir=f\"./logs/{run_prefix}/{run_name}\",\n",
    "        run_name=run_name,\n",
    "        report_to=\"wandb\",\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3cd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_bert(trial):\n",
    "    trainer = build_trainer(\n",
    "        model_checkpoint=\"bert-base-uncased\",\n",
    "        trial=trial,\n",
    "        run_prefix=\"bert\",\n",
    "        train_dataset=tokenized_bert_train_sub,\n",
    "        val_dataset=tokenized_bert_val_sub\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    return eval_result[\"eval_f1_macro\"]\n",
    "\n",
    "def objective_roberta(trial):\n",
    "    trainer = build_trainer(\n",
    "        model_checkpoint=\"roberta-base\",\n",
    "        trial=trial,\n",
    "        run_prefix=\"roberta\",\n",
    "        train_dataset=tokenized_roberta_train_sub,\n",
    "        val_dataset=tokenized_roberta_val_sub\n",
    "    )\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    return eval_result[\"eval_f1_macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4921af",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_bert = optuna.create_study(direction=\"maximize\",\n",
    "                                 pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
    "                                 study_name=\"bert_study_stratify_no_preprocess\",\n",
    "                                 storage=\"sqlite:////content/drive/MyDrive/Colab Notebooks/nlp_project/optuna/bert_study_stratify_no_preprocess.db\",\n",
    "                                 load_if_exists=True)\n",
    "study_bert.optimize(objective_bert, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3092247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_roberta = optuna.create_study(direction=\"maximize\",\n",
    "                                    pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
    "                                    study_name=\"roberta_study_stratify_no_preprocess\",\n",
    "                                    storage=\"sqlite:////content/drive/MyDrive/Colab Notebooks/nlp_project/optuna/roberta_study_stratify_no_preprocess.db\",\n",
    "                                    load_if_exists=True)\n",
    "study_roberta.optimize(objective_roberta, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1abfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_bert = study_bert.best_trial\n",
    "print('Bert best trial on subset:')\n",
    "print(best_trial_bert.params)\n",
    "best_trial_roberta = study_roberta.best_trial\n",
    "print('RoBerta best trial on subset:')\n",
    "print(best_trial_roberta.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2aefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trainer_bert = build_trainer(\n",
    "    model_checkpoint=\"bert-base-uncased\",\n",
    "    trial=best_trial_bert,\n",
    "    run_prefix=\"bert_final_stratify_no_preprocess\",\n",
    "    train_dataset=tokenized_bert_train,\n",
    "    val_dataset=tokenized_bert_val\n",
    ")\n",
    "final_trainer_bert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc052fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trainer_roberta = build_trainer(\n",
    "    model_checkpoint=\"roberta-base\",\n",
    "    trial=best_trial_roberta,\n",
    "    run_prefix=\"roberta_final_stratify_no_preprocess\",\n",
    "    train_dataset=tokenized_roberta_train,\n",
    "    val_dataset=tokenized_roberta_val\n",
    ")\n",
    "final_trainer_roberta.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trainer_bert.save_model(\"models/bert_final_stratify_no_preprocess\")\n",
    "bert_tokenizer.save_pretrained(\"models/bert_final_stratify_no_preprocess\")\n",
    "!cp -r models/bert_final_stratify_no_preprocess \"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/bert_best_model_stratify_no_preprocess\"\n",
    "\n",
    "final_trainer_roberta.save_model(\"models/roberta_final_stratify_no_preprocess\")\n",
    "roberta_tokenizer.save_pretrained(\"models/roberta_final_stratify_no_preprocess\")\n",
    "!cp -r models/roberta_final_stratify_no_preprocess \"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/roberta_best_model_stratify_no_preprocess\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
