{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5728f352",
      "metadata": {
        "id": "5728f352"
      },
      "source": [
        "## **NLP - Text Classification Project**\n",
        "Group H - August 2025\n",
        "\n",
        "Classification of tweets from Twitter that have been manually tagged for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a7e16b9",
      "metadata": {
        "id": "2a7e16b9"
      },
      "outputs": [],
      "source": [
        "# # Needed for Google Colab\n",
        "# !pip install --quiet evaluate transformers optuna datasets nltk scikit-learn\n",
        "# !pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98d8c7ec",
      "metadata": {
        "id": "98d8c7ec"
      },
      "outputs": [],
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from optuna.pruners import MedianPruner\n",
        "from scipy.stats import pearsonr\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import glob\n",
        "import nltk\n",
        "import evaluate\n",
        "import transformers\n",
        "import torch\n",
        "import optuna\n",
        "import wandb\n",
        "wandb.login()\n",
        "# API key - 0cbd7fe3cffd71df993b30edb4fa0db94f114413\n",
        "\n",
        "num_train_samples = 5000\n",
        "os.environ[\"WANDB_PROJECT\"] = f\"tweet-sentiment-classification_split_to_test_maxl_256_{num_train_samples}_samples_optuna\"\n",
        "os.environ[\"WANDB_INIT_TIMEOUT\"] = \"180\"\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7cb67fc",
      "metadata": {
        "id": "a7cb67fc"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_train.csv', encoding='latin1')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/nlp_project/Data/Corona_NLP_test.csv', encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f61f969",
      "metadata": {
        "id": "4f61f969"
      },
      "outputs": [],
      "source": [
        "# Merge and shuffle for better stratified splits\n",
        "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
        "df_full = df_full.sample(frac=1.0, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028f623d",
      "metadata": {
        "id": "028f623d"
      },
      "source": [
        "### **Pre-processing the Data**\n",
        "\n",
        "Three pre-processing strategies were considered:<br>\n",
        "1) No pre-processing – using tweets in their original form.<br>\n",
        "2) Minimal pre-processing – removing extra spaces.<br>\n",
        "3) Full pre-processing – lowercasing text, removing URLs, mentions, hashtags, special characters, and extra whitespace.\n",
        "\n",
        "The no pre-processing approach was selected, as it achieved the highest performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25abd4fe",
      "metadata": {
        "id": "25abd4fe"
      },
      "outputs": [],
      "source": [
        "# Try without pre-processing\n",
        "is_preprocessed = \"no_preprocess\"\n",
        "df_full[\"clean_text\"] = df_full[\"OriginalTweet\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Try minimal pre-processing\n",
        "# def light_preprocess(text):\n",
        "#     return text.strip()                             # Remove unnecessary spaces\n",
        "\n",
        "# is_preprocessed = \"minimal_preprocess\"\n",
        "# df_full[\"clean_text\"] = df_full[\"OriginalTweet\"].apply(light_preprocess)"
      ],
      "metadata": {
        "id": "9jbagrO7K5a5"
      },
      "id": "9jbagrO7K5a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Try pre-processing\n",
        "# def clean_text(text):\n",
        "#     text = str(text).lower()\n",
        "#     text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # URLs\n",
        "#     text = re.sub(r'\\@\\w+|\\#','', text)  # Mentions & hashtags\n",
        "#     text = re.sub(r'\\n', ' ', text)  # Line breaks\n",
        "#     text = re.sub(r\"[^a-zA-Z']\", ' ', text)  # Keep letters only\n",
        "#     text = re.sub(r'\\s+', ' ', text).strip()  # Extra whitespace\n",
        "#     return text\n",
        "\n",
        "# is_preprocessed = \"w_preprocess\"\n",
        "# df_full[\"clean_text\"] = df_full[\"OriginalTweet\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "eQ-pWe0v5T3F"
      },
      "id": "eQ-pWe0v5T3F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fc95eef6",
      "metadata": {
        "id": "fc95eef6"
      },
      "source": [
        "**Encode Sentiment Labels**\n",
        "\n",
        "Map each unique sentiment label to a numeric ID for model compatibility, and apply this mapping datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad49ba34",
      "metadata": {
        "id": "ad49ba34"
      },
      "outputs": [],
      "source": [
        "# Mapping sentiments to unique numeric IDs\n",
        "unique_labels = sorted(df_full[\"Sentiment\"].unique())\n",
        "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "df_full[\"label\"] = df_full[\"Sentiment\"].map(label2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stratified Data Splitting\n",
        "The dataset is split into train (70%), validation (15%), and test (15%) sets using stratified sampling to preserve the original label distribution across all subsets."
      ],
      "metadata": {
        "id": "CbcS9rIPTbyv"
      },
      "id": "CbcS9rIPTbyv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3bc653",
      "metadata": {
        "id": "1b3bc653"
      },
      "outputs": [],
      "source": [
        "# Stratified split: 70% train, 15% val, 15% test\n",
        "train_val_df, test_df = train_test_split(df_full, test_size=0.15, stratify=df_full[\"label\"], random_state=42)\n",
        "train_df, val_df = train_test_split(train_val_df, test_size=0.1765, stratify=train_val_df[\"label\"], random_state=42)\n",
        "\n",
        "# Confirm sizes\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Val size:\", len(val_df))\n",
        "print(\"Test size:\", len(test_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9d2aae",
      "metadata": {
        "id": "8a9d2aae"
      },
      "source": [
        "**Use Small Subsets for Quick Evaluation**\n",
        "\n",
        "Select shuffled samples from each training and validation dataset for both BERT and RoBERTa. This allows faster experimentation during model development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52bd0bc9",
      "metadata": {
        "id": "52bd0bc9"
      },
      "outputs": [],
      "source": [
        "train_subset_df, _ = train_test_split(\n",
        "    train_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=num_train_samples,\n",
        "    stratify=train_df[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_subset_df, _ = train_test_split(\n",
        "    val_df[[\"clean_text\", \"label\"]],\n",
        "    train_size=500,\n",
        "    stratify=val_df[\"label\"],\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36e48007",
      "metadata": {
        "id": "36e48007"
      },
      "source": [
        "## Fine-Tuning Pretrained Language Models\n",
        "\n",
        "Fine-tuning two pretrained transformer-based models from the Hugging Face library — BERT and RoBERTa — on our sentiment classification task. These models will be trained using the Hugging Face API. Model performance will be monitored and tuned using hyperparameter optimization (Optuna) and experiment tracking (Weights & Biases)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f809a00",
      "metadata": {
        "id": "5f809a00"
      },
      "source": [
        "**Load Pretrained Models**\n",
        "\n",
        "Initialize tokenizers and models for BERT and RoBERTa, both widely used transformer architectures for text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2655478",
      "metadata": {
        "id": "c2655478"
      },
      "outputs": [],
      "source": [
        "# Choose pretrained models\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "roberta_model_name = \"roberta-base\"\n",
        "\n",
        "sentiment_labels = df_full['Sentiment'].unique()\n",
        "n_labels = len(sentiment_labels)\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=n_labels)\n",
        "\n",
        "# Load RoBERTa tokenizer and model\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name, num_labels=n_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6baec2",
      "metadata": {
        "id": "1f6baec2"
      },
      "source": [
        "**Tokenization**\n",
        "\n",
        "Custom tokenization functions are defined for each model. The dataset splits are converted to Hugging Face Dataset objects and tokenized separately for BERT and RoBERTa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68b523d",
      "metadata": {
        "id": "e68b523d"
      },
      "outputs": [],
      "source": [
        "# Tokenize function\n",
        "def tokenize_function_bert(examples):\n",
        "    return bert_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "def tokenize_function_roberta(examples):\n",
        "    return roberta_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab47dda3",
      "metadata": {
        "id": "ab47dda3"
      },
      "outputs": [],
      "source": [
        "# Convert DataFrame to Hugging Face Dataset\n",
        "hf_subset_train = Dataset.from_pandas(train_subset_df)\n",
        "hf_subset_val = Dataset.from_pandas(val_subset_df)\n",
        "\n",
        "hf_train = Dataset.from_pandas(train_df[[\"clean_text\", \"label\"]])\n",
        "hf_val = Dataset.from_pandas(val_df[[\"clean_text\", \"label\"]])\n",
        "hf_test = Dataset.from_pandas(test_df[[\"clean_text\", \"label\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b99bc94",
      "metadata": {
        "id": "6b99bc94"
      },
      "outputs": [],
      "source": [
        "# Tokenize subsets\n",
        "# Tokenize for BERT\n",
        "tokenized_bert_train_sub = hf_subset_train.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_train_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_bert_val_sub = hf_subset_val.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_val_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Tokenize for RoBERTa\n",
        "tokenized_roberta_train_sub = hf_subset_train.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_train_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_val_sub = hf_subset_val.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_val_sub.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1efa62d8",
      "metadata": {
        "id": "1efa62d8"
      },
      "outputs": [],
      "source": [
        "# Tokenize full dataset\n",
        "# Tokenize for BERT\n",
        "tokenized_bert_train = hf_train.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_bert_val = hf_val.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_bert_test = hf_test.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Tokenize for RoBERTa\n",
        "tokenized_roberta_train = hf_train.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_val = hf_val.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_test = hf_test.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric Computation**\n",
        "\n",
        "The evaluation metrics include accuracy and macro-averaged F1 score. The compute_metrics function converts model logits into predicted labels and calculates both metrics, providing a balanced performance measure across all classes."
      ],
      "metadata": {
        "id": "YnnJzS1eUwoo"
      },
      "id": "YnnJzS1eUwoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90b7dbf6",
      "metadata": {
        "id": "90b7dbf6"
      },
      "outputs": [],
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"],\n",
        "        \"f1_macro\": f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"],\n",
        "        \"precision_macro\": precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"],\n",
        "        \"recall_macro\":    recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eddf48fe",
      "metadata": {
        "id": "eddf48fe"
      },
      "source": [
        "**Hyperparameters Tuning with Optuna**\n",
        "\n",
        "Hyperparameter tuning for BERT and RoBERTa using Optuna.\n",
        "A function configuring Hugging Face Trainer with parameters suggested by Optuna, including learning rate, batch size, and number of epochs. Each model is trained and evaluated on a subset of the dataset, with macro F1 score as the optimization target. Results are logged to Weights & Biases, and Optuna’s Median Pruner is used to stop underperforming trials early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7b5654",
      "metadata": {
        "id": "0d7b5654"
      },
      "outputs": [],
      "source": [
        "def build_trainer(model_checkpoint, trial, run_prefix, train_dataset, val_dataset):\n",
        "    # Sample hyperparameters\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
        "    num_epochs = trial.suggest_int(\"num_train_epochs\", 2, 5)\n",
        "    n_samples = len(train_dataset)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=5)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    run_name = f\"{run_prefix}-ep{num_epochs}-lr{learning_rate}-bs{batch_size}-samples{n_samples}-run{int(time.time())}-{is_preprocessed}\"\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"./results/{run_prefix}/{run_name}\",\n",
        "        disable_tqdm=True,\n",
        "        fp16=True,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        weight_decay=0.01,\n",
        "        label_smoothing_factor=0.1,\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=1,\n",
        "        logging_strategy=\"epoch\",\n",
        "        logging_dir=f\"./logs/{run_prefix}/{run_name}\",\n",
        "        run_name=run_name,\n",
        "        report_to=\"wandb\",\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed3cd8cc",
      "metadata": {
        "id": "ed3cd8cc"
      },
      "outputs": [],
      "source": [
        "def objective_bert(trial):\n",
        "    trainer = build_trainer(\n",
        "        model_checkpoint=\"bert-base-uncased\",\n",
        "        trial=trial,\n",
        "        run_prefix=\"bert\",\n",
        "        train_dataset=tokenized_bert_train_sub,\n",
        "        val_dataset=tokenized_bert_val_sub\n",
        "    )\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    wandb.finish()\n",
        "    return eval_result[\"eval_f1_macro\"]\n",
        "\n",
        "def objective_roberta(trial):\n",
        "    trainer = build_trainer(\n",
        "        model_checkpoint=\"roberta-base\",\n",
        "        trial=trial,\n",
        "        run_prefix=\"roberta\",\n",
        "        train_dataset=tokenized_roberta_train_sub,\n",
        "        val_dataset=tokenized_roberta_val_sub\n",
        "    )\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "    wandb.finish()\n",
        "    return eval_result[\"eval_f1_macro\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4921af",
      "metadata": {
        "id": "fb4921af"
      },
      "outputs": [],
      "source": [
        "study_bert = optuna.create_study(direction=\"maximize\",\n",
        "                                 pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
        "                                 study_name=f\"bert_study_stratify_{is_preprocessed}\",\n",
        "                                 storage=f\"sqlite:////content/drive/MyDrive/Colab Notebooks/nlp_project/optuna/bert_study_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna.db\",\n",
        "                                 load_if_exists=True)\n",
        "study_bert.optimize(objective_bert, n_trials=5)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3092247f",
      "metadata": {
        "id": "3092247f"
      },
      "outputs": [],
      "source": [
        "study_roberta = optuna.create_study(direction=\"maximize\",\n",
        "                                    pruner=MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
        "                                    study_name=f\"roberta_study_stratify_{is_preprocessed}\",\n",
        "                                    storage=f\"sqlite:////content/drive/MyDrive/Colab Notebooks/nlp_project/optuna/roberta_study_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna.db\",\n",
        "                                    load_if_exists=True)\n",
        "study_roberta.optimize(objective_roberta, n_trials=5)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display best trial parameters"
      ],
      "metadata": {
        "id": "HrqHx3gLVkD-"
      },
      "id": "HrqHx3gLVkD-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1abfc1",
      "metadata": {
        "id": "ac1abfc1"
      },
      "outputs": [],
      "source": [
        "best_trial_bert = study_bert.best_trial\n",
        "print('Bert best trial on subset:')\n",
        "print(best_trial_bert.params)\n",
        "best_trial_roberta = study_roberta.best_trial\n",
        "print('RoBerta best trial on subset:')\n",
        "print(best_trial_roberta.params)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Model Training with Best Hyperparameters\n",
        "The best hyperparameters from the Optuna search are used to retrain BERT and RoBERTa on the full training set.\n",
        "Each model is trained and evaluated using the Hugging Face Trainer with W&B logging.\n"
      ],
      "metadata": {
        "id": "m2zlCnYZVz2O"
      },
      "id": "m2zlCnYZVz2O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2aefb5",
      "metadata": {
        "id": "fd2aefb5"
      },
      "outputs": [],
      "source": [
        "best_params_bert = best_trial_bert.params\n",
        "run_name_bert = f\"bert_final_stratify_{is_preprocessed}-ep{best_params_bert['num_train_epochs']}-lr{best_params_bert['learning_rate']:.1e}-bs{best_params_bert['batch_size']}\"\n",
        "wandb.init(project=f\"tweet-sentiment-classification_split_to_test_maxl_256_{num_train_samples}_samples_optuna\", name=run_name_bert, reinit=True)\n",
        "\n",
        "final_trainer_bert = build_trainer(\n",
        "    model_checkpoint=\"bert-base-uncased\",\n",
        "    trial=best_trial_bert,\n",
        "    run_prefix=f\"bert_final_stratify_{is_preprocessed}\",\n",
        "    train_dataset=tokenized_bert_train,\n",
        "    val_dataset=tokenized_bert_val\n",
        ")\n",
        "final_trainer_bert.train()\n",
        "final_trainer_bert.evaluate(tokenized_bert_test)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccc052fa",
      "metadata": {
        "id": "ccc052fa"
      },
      "outputs": [],
      "source": [
        "best_params_roberta = best_trial_roberta.params\n",
        "run_name_roberta = f\"roberta_final_stratify_{is_preprocessed}-ep{best_params_roberta['num_train_epochs']}-lr{best_params_roberta['learning_rate']:.1e}-bs{best_params_roberta['batch_size']}\"\n",
        "wandb.init(project=f\"tweet-sentiment-classification_split_to_test_maxl_256_{num_train_samples}_samples_optuna\", name=run_name_roberta, reinit=True)\n",
        "\n",
        "final_trainer_roberta = build_trainer(\n",
        "    model_checkpoint=\"roberta-base\",\n",
        "    trial=best_trial_roberta,\n",
        "    run_prefix=f\"roberta_final_stratify_{is_preprocessed}\",\n",
        "    train_dataset=tokenized_roberta_train,\n",
        "    val_dataset=tokenized_roberta_val\n",
        ")\n",
        "final_trainer_roberta.train()\n",
        "final_trainer_roberta.evaluate(tokenized_roberta_test)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d203d986",
      "metadata": {
        "id": "d203d986"
      },
      "outputs": [],
      "source": [
        "# Saving final models\n",
        "final_trainer_bert.save_model(f\"models/bert_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna\")\n",
        "bert_tokenizer.save_pretrained(f\"models/bert_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna\")\n",
        "!cp -r models/bert_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna \"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/bert_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "\n",
        "final_trainer_roberta.save_model(f\"models/roberta_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna\")\n",
        "roberta_tokenizer.save_pretrained(f\"models/roberta_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna\")\n",
        "!cp -r models/roberta_final_stratify_{is_preprocessed}_{num_train_samples}_samples_optuna \"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/roberta_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ensemble Models**"
      ],
      "metadata": {
        "id": "mIMTD46Y5kId"
      },
      "id": "mIMTD46Y5kId"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, combining predictions from multiple fine-tuned transformer models (BERT, RoBERTa, and DeBERTa) using soft-vote ensembling. Instead of relying on a single model’s predictions, averaging the class probabilities from each model—optionally with optimized weights via Optuna—to leverage their complementary strengths and reduce individual model biases, often leading to improved accuracy and robustness."
      ],
      "metadata": {
        "id": "y2xKZicaC-FO"
      },
      "id": "y2xKZicaC-FO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Class Probabilities"
      ],
      "metadata": {
        "id": "mTLZCgHuPmrO"
      },
      "id": "mTLZCgHuPmrO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts, run in eval mode and outputs softmax probabilities for each class\n",
        "def model_probs(model, tokenizer, texts, batch_size=32, max_length=256):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    ds = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    dl = DataLoader(ds, batch_size=batch_size)\n",
        "\n",
        "    probs_all = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attn_mask in dl:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attn_mask = attn_mask.to(device)\n",
        "            logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
        "            probs_all.append(probs)\n",
        "    return np.concatenate(probs_all, axis=0)"
      ],
      "metadata": {
        "id": "XOJGBTmaBawj"
      },
      "id": "XOJGBTmaBawj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Fine-Tuned Models and Generate Predictions"
      ],
      "metadata": {
        "id": "NJaivwehPb1E"
      },
      "id": "NJaivwehPb1E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best finetuned checkpoints\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "best_bert_path = f\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/bert_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(best_bert_path).to(device)\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(best_bert_path)\n",
        "\n",
        "best_roberta_path = f\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/roberta_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(best_roberta_path).to(device)\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(best_roberta_path)\n",
        "\n",
        "best_deberta_path = f\"/content/drive/MyDrive/Colab Notebooks/nlp_project/models/w_test_split/deberta_best_model_stratify_maxl_128_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "deberta_model = AutoModelForSequenceClassification.from_pretrained(best_deberta_path).to(device)\n",
        "deberta_tokenizer = AutoTokenizer.from_pretrained(best_deberta_path)\n",
        "\n",
        "val_texts = list(val_df[\"clean_text\"])\n",
        "val_labels = val_df[\"label\"].values\n",
        "\n",
        "test_texts = list(test_df[\"clean_text\"])\n",
        "test_labels = test_df[\"label\"].values\n",
        "\n",
        "# Generate class probability predictions on validation and test\n",
        "bert_val_probs   = model_probs(bert_model, bert_tokenizer, val_texts,  batch_size=32, max_length=256)\n",
        "roberta_val_probs= model_probs(roberta_model, roberta_tokenizer, val_texts, batch_size=32, max_length=256)\n",
        "deberta_val_probs= model_probs(deberta_model, deberta_tokenizer, val_texts, batch_size=32, max_length=128)\n",
        "\n",
        "bert_test_probs    = model_probs(bert_model, bert_tokenizer, test_texts,  batch_size=32, max_length=256)\n",
        "roberta_test_probs = model_probs(roberta_model, roberta_tokenizer, test_texts, batch_size=32, max_length=256)\n",
        "deberta_test_probs= model_probs(deberta_model, deberta_tokenizer, test_texts, batch_size=32, max_length=128)\n"
      ],
      "metadata": {
        "id": "MeUMooRNDPiD"
      },
      "id": "MeUMooRNDPiD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weighted Ensemble with Grid Search\n",
        "Searching for the optimal combination of model weights using a grid search on the validation set, aiming to maximize the macro-F1 score. The best weights are then applied to the test set to evaluate the final ensemble performance."
      ],
      "metadata": {
        "id": "jVVcKDE9QsJi"
      },
      "id": "jVVcKDE9QsJi"
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_probs(probs_list, weights):\n",
        "    w = np.array(weights, dtype=np.float32)\n",
        "    w = w / w.sum()\n",
        "    out = np.zeros_like(probs_list[0], dtype=np.float32)\n",
        "    for wi, pi in zip(w, probs_list):\n",
        "        out += wi * pi\n",
        "    return out\n",
        "\n",
        "def metrics_from_probs(probs, y_true):\n",
        "    preds = probs.argmax(axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, preds),\n",
        "        \"macro_f1\": f1_score(y_true, preds, average=\"macro\")\n",
        "    }\n",
        "\n",
        "def grid_search_weights_val(probs_list, y_true, step=0.05):\n",
        "    n = len(probs_list)\n",
        "    best = {\"weights\": None, \"macro_f1\": -1.0, \"accuracy\": 0.0}\n",
        "    grid = np.arange(0.0, 1.0 + 1e-9, step)\n",
        "\n",
        "    if n == 2:\n",
        "        for w0 in grid:\n",
        "            w = [w0, 1.0 - w0]\n",
        "            scores = metrics_from_probs(ensemble_probs(probs_list, w), y_true)\n",
        "            if scores[\"macro_f1\"] > best[\"macro_f1\"]:\n",
        "                best = {\"weights\": w, **scores}\n",
        "        return best\n",
        "\n",
        "    if n == 3:\n",
        "        for w0 in grid:\n",
        "            for w1 in grid:\n",
        "                s = w0 + w1\n",
        "                if s <= 1.0 + 1e-9:\n",
        "                    w2 = 1.0 - s\n",
        "                    w = [w0, w1, w2]\n",
        "                    scores = metrics_from_probs(ensemble_probs(probs_list, w), y_true)\n",
        "                    if scores[\"macro_f1\"] > best[\"macro_f1\"]:\n",
        "                        best = {\"weights\": w, **scores}\n",
        "        return best\n",
        "\n",
        "    raise ValueError(\"Only supports 2 or 3 models in this helper.\")\n",
        "\n",
        "# Lists for 3-model case (BERT + RoBERTa + DeBerta)\n",
        "val_probs_list  = [bert_val_probs, roberta_val_probs, deberta_val_probs]\n",
        "test_probs_list = [bert_test_probs, roberta_test_probs, deberta_test_probs]\n",
        "\n",
        "# Grid search on validation\n",
        "best = grid_search_weights_val(val_probs_list, val_labels, step=0.05)\n",
        "print(\"Best weights on validation:\", best)\n",
        "\n",
        "# Apply best weights to test\n",
        "ens_test_probs = ensemble_probs(test_probs_list, best[\"weights\"])\n",
        "test_scores = metrics_from_probs(ens_test_probs, test_labels)\n",
        "\n",
        "print(f\" Test (weighted ensemble) Accuracy: {test_scores['accuracy']:.4f}\")\n",
        "print(f\" Test (weighted ensemble) Macro-F1: {test_scores['macro_f1']:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bYk6tNXUDjzW"
      },
      "id": "bYk6tNXUDjzW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Ensemble Evaluation\n",
        "Producing classification metrics (precision, recall, and F1-score) for each sentiment class. In addition, visualizing confusion matrix to show where the model performs well and where it confuses between classes."
      ],
      "metadata": {
        "id": "vV5-SVbXRE1A"
      },
      "id": "vV5-SVbXRE1A"
    },
    {
      "cell_type": "code",
      "source": [
        "# Final predictions from ensemble\n",
        "ens_test_preds = ens_test_probs.argmax(axis=1)\n",
        "\n",
        "# Class labels\n",
        "class_labels = [\n",
        "    \"Extremely Negative\",\n",
        "    \"Negative\",\n",
        "    \"Neutral\",\n",
        "    \"Positive\",\n",
        "    \"Extremely Positive\"\n",
        "]\n",
        "\n",
        "# Per-class F1, Precision, Recall\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, ens_test_preds, target_names=class_labels, digits=4))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, ens_test_preds)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - Final Weighted Ensemble\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "INwPz017Dy_W"
      },
      "id": "INwPz017Dy_W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pairwise Model Agreement Analysis\n",
        "Comparing predictions of each model pair (BERT, RoBERTa, and DeBERTa) on the test set. Reporting agreement rate, Cohen’s κ (chance-adjusted agreement), and prediction correlation, helping to assess model diversity."
      ],
      "metadata": {
        "id": "HSejKuLoRn79"
      },
      "id": "HSejKuLoRn79"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert probs -> hard predictions for test\n",
        "bert_test_preds    = bert_test_probs.argmax(axis=1)\n",
        "roberta_test_preds = roberta_test_probs.argmax(axis=1)\n",
        "deberta_test_preds = deberta_test_probs.argmax(axis=1)\n",
        "\n",
        "def pairwise_stats(name1, p1, name2, p2):\n",
        "    agree = accuracy_score(p1, p2)\n",
        "    kappa = cohen_kappa_score(p1, p2)\n",
        "    corr, _ = pearsonr(p1, p2)\n",
        "    print(f\"{name1} vs {name2}:\")\n",
        "    print(f\"  • Agreement: {agree:.4f}\")\n",
        "    print(f\"  • Cohen's κ: {kappa:.4f}\")\n",
        "    print(f\"  • Pred. correlation: {corr:.4f}\\n\")\n",
        "\n",
        "pairwise_stats(\"BERT\",    bert_test_preds,    \"RoBERTa\", roberta_test_preds)\n",
        "pairwise_stats(\"BERT\",    bert_test_preds,    \"DeBERTa\", deberta_test_preds)\n",
        "pairwise_stats(\"RoBERTa\", roberta_test_preds, \"DeBERTa\", deberta_test_preds)"
      ],
      "metadata": {
        "id": "3MfF4VkGILgp"
      },
      "id": "3MfF4VkGILgp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o3JMrkSU3mKS"
      },
      "id": "o3JMrkSU3mKS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}