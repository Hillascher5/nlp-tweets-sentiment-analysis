{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPkm0wgEkaw9shwRACDK4Aw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hillascher5/nlp-tweets-sentiment-analysis/blob/main/Evaluate_saved_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Saved Best Models\n",
        "\n",
        "Evaluate models of all types, From Trainer API and manual code models."
      ],
      "metadata": {
        "id": "1Q3qAkioirrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Needed for Google Colab\n",
        "# !pip install --quiet evaluate transformers optuna datasets nltk scikit-learn\n",
        "# !pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "Ms9nSLXui351"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "0Pbbu3fPi1hV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed_all(42)"
      ],
      "metadata": {
        "id": "lFo_w1F-05BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load test set"
      ],
      "metadata": {
        "id": "35ugAZ8xszgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"data/Corona_NLP_test_to_evaluate.csv\", encoding='latin1')"
      ],
      "metadata": {
        "id": "bUNFTiNusbUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sentiments to unique numeric IDs\n",
        "unique_labels = sorted(df_test[\"Sentiment\"].unique())\n",
        "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "df_test[\"label\"] = df_test[\"Sentiment\"].map(label2id)"
      ],
      "metadata": {
        "id": "DPDKK2DRrxuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimal pre-processing\n",
        "def light_preprocess(text):\n",
        "    return text.strip()                             # Remove unnecessary spaces\n",
        "\n",
        "is_preprocessed = \"minimal_preprocess\"\n",
        "df_test[\"clean_text\"] = df_test[\"OriginalTweet\"].apply(light_preprocess)"
      ],
      "metadata": {
        "id": "4Tb3ls__rcBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DataFrame to Hugging Face Dataset\n",
        "hf_test = Dataset.from_pandas(df_test[[\"clean_text\", \"label\"]])"
      ],
      "metadata": {
        "id": "GWu23kabuQtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load saved models"
      ],
      "metadata": {
        "id": "b-CzfihzsvnR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrw-6thcinlC"
      },
      "outputs": [],
      "source": [
        "# model_path_hf = \"models/HF_Trainer/\"\n",
        "model_path_hf = \"/content/drive/MyDrive/NLP_tweet_classification_covid19_project/models/HF_Trainer/\"\n",
        "\n",
        "bert_model_name_hf = \"bert_best_model_stratify_maxl_256_minimal_preprocess_5000_samples_optuna\"\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(model_path_hf + bert_model_name_hf)\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_path_hf + bert_model_name_hf)\n",
        "\n",
        "roberta_model_name_hf = \"roberta_best_model_stratify_maxl_256_minimal_preprocess_5000_samples_optuna\"\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(model_path_hf + roberta_model_name_hf)\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(model_path_hf + roberta_model_name_hf)\n",
        "\n",
        "deberta_model_name_hf = \"deberta_best_model_stratify_maxl_128_minimal_preprocess_5000_samples_optuna\"\n",
        "deberta_model = AutoModelForSequenceClassification.from_pretrained(model_path_hf + deberta_model_name_hf)\n",
        "deberta_tokenizer = AutoTokenizer.from_pretrained(model_path_hf + deberta_model_name_hf)\n",
        "\n",
        "# model_path_manual = \"models/Manual_finetune_min_preproc_5000_samples_opt/\"\n",
        "model_path_manual = \"/content/drive/MyDrive/NLP_tweet_classification_covid19_project/models/Manual_finetune_min_preproc_5000_samples_opt/\"\n",
        "\n",
        "bert_model_name_manual = \"manual2hf_bert-base-uncased_5000_samples_opt\"\n",
        "bert_model_man = AutoModelForSequenceClassification.from_pretrained(model_path_manual + bert_model_name_manual)\n",
        "bert_tokenizer_man = AutoTokenizer.from_pretrained(model_path_manual + bert_model_name_manual)\n",
        "\n",
        "roberta_model_name_manual = \"manual2hf_roberta-base_5000_samples_opt\"\n",
        "roberta_model_man = AutoModelForSequenceClassification.from_pretrained(model_path_manual + roberta_model_name_manual)\n",
        "roberta_tokenizer_man = AutoTokenizer.from_pretrained(model_path_manual + roberta_model_name_manual)\n",
        "\n",
        "deberta_model_name_manual = \"manual2hf_deberta-base_5000_samples_opt\"\n",
        "deberta_model_man = AutoModelForSequenceClassification.from_pretrained(model_path_manual + deberta_model_name_manual)\n",
        "deberta_tokenizer_man = AutoTokenizer.from_pretrained(model_path_manual + deberta_model_name_manual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize function\n",
        "def tokenize_function_bert(examples):\n",
        "    return bert_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "def tokenize_function_roberta(examples):\n",
        "    return roberta_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "def tokenize_function_deberta(examples):\n",
        "    return deberta_tokenizer(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "def tokenize_function_bert_man(examples):\n",
        "    return bert_tokenizer_man(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "def tokenize_function_roberta_man(examples):\n",
        "    return roberta_tokenizer_man(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=256)\n",
        "\n",
        "def tokenize_function_deberta_man(examples):\n",
        "    return deberta_tokenizer_man(examples[\"clean_text\"], truncation=True, padding='max_length', max_length=128)"
      ],
      "metadata": {
        "id": "HgyVQJIttpsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "tokenized_bert_test = hf_test.map(tokenize_function_bert, batched=True)\n",
        "tokenized_bert_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_test = hf_test.map(tokenize_function_roberta, batched=True)\n",
        "tokenized_roberta_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_deberta_test = hf_test.map(tokenize_function_deberta, batched=True)\n",
        "tokenized_deberta_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_bert_test_man = hf_test.map(tokenize_function_bert_man, batched=True)\n",
        "tokenized_bert_test_man.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_roberta_test_man = hf_test.map(tokenize_function_roberta_man, batched=True)\n",
        "tokenized_roberta_test_man.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "tokenized_deberta_test_man = hf_test.map(tokenize_function_deberta_man, batched=True)\n",
        "tokenized_deberta_test_man.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "lKX49Rw6uJyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_score(labels, predictions),\n",
        "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
        "        'precision_macro': precision_score(labels, predictions, average='macro', zero_division=0),\n",
        "        'recall_macro': recall_score(labels, predictions, average='macro'),\n",
        "    }"
      ],
      "metadata": {
        "id": "tHLzvMvzzcEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define trainers\n",
        "bert_trainer_hf = Trainer(model=bert_model, tokenizer=bert_tokenizer, compute_metrics=compute_metrics)\n",
        "roberta_trainer_hf = Trainer(model=roberta_model, tokenizer=roberta_tokenizer, compute_metrics=compute_metrics)\n",
        "deberta_trainer_hf = Trainer(model=deberta_model, tokenizer=deberta_tokenizer, compute_metrics=compute_metrics)\n",
        "\n",
        "bert_trainer_man = Trainer(model=bert_model_man, tokenizer=bert_tokenizer_man, compute_metrics=compute_metrics)\n",
        "roberta_trainer_man = Trainer(model=roberta_model_man, tokenizer=roberta_tokenizer_man, compute_metrics=compute_metrics)\n",
        "deberta_trainer_man = Trainer(model=deberta_model_man, tokenizer=deberta_tokenizer_man, compute_metrics=compute_metrics)"
      ],
      "metadata": {
        "id": "69QfFsxsu2Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "bert_metrics = bert_trainer_hf.evaluate(tokenized_bert_test)\n",
        "print(bert_metrics)\n",
        "\n",
        "roberta_metrics = roberta_trainer_hf.evaluate(tokenized_roberta_test)\n",
        "print(roberta_metrics)\n",
        "\n",
        "deberta_metrics = deberta_trainer_hf.evaluate(tokenized_deberta_test)\n",
        "print(deberta_metrics)\n",
        "\n",
        "bert_metrics_man = bert_trainer_man.evaluate(tokenized_bert_test_man)\n",
        "print(bert_metrics_man)\n",
        "\n",
        "roberta_metrics_man = roberta_trainer_man.evaluate(tokenized_roberta_test_man)\n",
        "print(roberta_metrics_man)\n",
        "\n",
        "deberta_metrics_man = deberta_trainer_man.evaluate(tokenized_deberta_test_man)\n",
        "print(deberta_metrics_man)"
      ],
      "metadata": {
        "id": "FPfk6l15v2nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define which metrics to keep\n",
        "keys_to_include = [\n",
        "    'eval_loss',\n",
        "    'eval_accuracy',\n",
        "    'eval_f1_macro',\n",
        "    'eval_precision_macro',\n",
        "    'eval_recall_macro'\n",
        "]\n",
        "\n",
        "# Organize metrics\n",
        "metrics_table = pd.DataFrame([\n",
        "    {\"Model\": \"BERT (HF)\", **{k: v for k, v in bert_metrics.items() if k in keys_to_include}},\n",
        "    {\"Model\": \"RoBERTa (HF)\", **{k: v for k, v in roberta_metrics.items() if k in keys_to_include}},\n",
        "    {\"Model\": \"DeBERTa (HF)\", **{k: v for k, v in deberta_metrics.items() if k in keys_to_include}},\n",
        "    {\"Model\": \"BERT (Manual)\", **{k: v for k, v in bert_metrics_man.items() if k in keys_to_include}},\n",
        "    {\"Model\": \"RoBERTa (Manual)\", **{k: v for k, v in roberta_metrics_man.items() if k in keys_to_include}},\n",
        "    {\"Model\": \"DeBERTa (Manual)\", **{k: v for k, v in deberta_metrics_man.items() if k in keys_to_include}},\n",
        "])\n",
        "\n",
        "# Set index to model name for clarity\n",
        "metrics_table.set_index(\"Model\", inplace=True)\n",
        "\n",
        "# Round values for better readability\n",
        "metrics_table = metrics_table.round(4)\n",
        "\n",
        "from IPython.display import display\n",
        "display(metrics_table)"
      ],
      "metadata": {
        "id": "2nkPL0_oHcKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions for classification report\n",
        "bert_preds = bert_trainer_hf.predict(tokenized_bert_test)\n",
        "bert_y_pred = bert_preds.predictions.argmax(axis=-1)\n",
        "bert_y_true = bert_preds.label_ids\n",
        "print('BERT HF Trainer model:')\n",
        "print(classification_report(bert_y_true, bert_y_pred, digits=4))\n",
        "\n",
        "roberta_preds = roberta_trainer_hf.predict(tokenized_roberta_test)\n",
        "roberta_y_pred = roberta_preds.predictions.argmax(axis=-1)\n",
        "roberta_y_true = roberta_preds.label_ids\n",
        "print('RoBERTa HF Trainer model:')\n",
        "print(classification_report(roberta_y_true, roberta_y_pred, digits=4))\n",
        "\n",
        "deberta_preds = deberta_trainer_hf.predict(tokenized_deberta_test)\n",
        "deberta_y_pred = deberta_preds.predictions.argmax(axis=-1)\n",
        "deberta_y_true = deberta_preds.label_ids\n",
        "print('DeBERTa HF Trainer model:')\n",
        "print(classification_report(deberta_y_true, deberta_y_pred, digits=4))\n",
        "\n",
        "bert_preds_man = bert_trainer_man.predict(tokenized_bert_test_man)\n",
        "bert_y_pred_man = bert_preds_man.predictions.argmax(axis=-1)\n",
        "bert_y_true_man = bert_preds_man.label_ids\n",
        "print('BERT manual code model:')\n",
        "print(classification_report(bert_y_true_man, bert_y_pred_man, digits=4))\n",
        "\n",
        "roberta_preds_man = roberta_trainer_man.predict(tokenized_roberta_test_man)\n",
        "roberta_y_pred_man = roberta_preds_man.predictions.argmax(axis=-1)\n",
        "roberta_y_true_man = roberta_preds_man.label_ids\n",
        "print('RoBERTA manual code model:')\n",
        "print(classification_report(roberta_y_true_man, roberta_y_pred_man, digits=4))\n",
        "\n",
        "deberta_preds_man = deberta_trainer_man.predict(tokenized_deberta_test_man)\n",
        "deberta_y_pred_man = deberta_preds_man.predictions.argmax(axis=-1)\n",
        "deberta_y_true_man = deberta_preds_man.label_ids\n",
        "print('DeBERTA manual code model:')\n",
        "print(classification_report(deberta_y_true_man, deberta_y_pred_man, digits=4))\n"
      ],
      "metadata": {
        "id": "HXl1r-lwv8QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Evaluation"
      ],
      "metadata": {
        "id": "LJlJy0VR5CP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize texts, run in eval mode and outputs softmax probabilities for each class\n",
        "def model_probs(model, tokenizer, texts, batch_size=32, max_length=256):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\n",
        "        list(texts),\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    ds = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "    dl = DataLoader(ds, batch_size=batch_size)\n",
        "\n",
        "    probs_all = []\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attn_mask in dl:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attn_mask = attn_mask.to(device)\n",
        "            logits = model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
        "            probs_all.append(probs)\n",
        "    return np.concatenate(probs_all, axis=0)"
      ],
      "metadata": {
        "id": "ezKiQUko5A4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best finetuned checkpoints\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "best_bert_path = f\"models/HF_Trainer/bert_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(best_bert_path).to(device)\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(best_bert_path)\n",
        "\n",
        "best_roberta_path = f\"models/HF_Trainer/roberta_best_model_stratify_maxl_256_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "roberta_model = AutoModelForSequenceClassification.from_pretrained(best_roberta_path).to(device)\n",
        "roberta_tokenizer = AutoTokenizer.from_pretrained(best_roberta_path)\n",
        "\n",
        "best_deberta_path = f\"models/HF_Trainer/deberta_best_model_stratify_maxl_128_{is_preprocessed}_{num_train_samples}_samples_optuna\"\n",
        "deberta_model = AutoModelForSequenceClassification.from_pretrained(best_deberta_path).to(device)\n",
        "deberta_tokenizer = AutoTokenizer.from_pretrained(best_deberta_path)\n",
        "\n",
        "val_texts = list(val_df[\"clean_text\"])\n",
        "val_labels = val_df[\"label\"].values\n",
        "\n",
        "test_texts = list(test_df[\"clean_text\"])\n",
        "test_labels = test_df[\"label\"].values\n",
        "\n",
        "# Generate class probability predictions on validation and test\n",
        "bert_val_probs= model_probs(bert_model, bert_tokenizer, val_texts,  batch_size=32, max_length=256)\n",
        "roberta_val_probs = model_probs(roberta_model, roberta_tokenizer, val_texts, batch_size=32, max_length=256)\n",
        "deberta_val_probs = model_probs(deberta_model, deberta_tokenizer, val_texts, batch_size=32, max_length=128)\n",
        "\n",
        "bert_test_probs = model_probs(bert_model, bert_tokenizer, test_texts,  batch_size=32, max_length=256)\n",
        "roberta_test_probs = model_probs(roberta_model, roberta_tokenizer, test_texts, batch_size=32, max_length=256)\n",
        "deberta_test_probs = model_probs(deberta_model, deberta_tokenizer, test_texts, batch_size=32, max_length=128)"
      ],
      "metadata": {
        "id": "Log8ljlY5Vs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_probs(probs_list, weights):\n",
        "    w = np.array(weights, dtype=np.float32)\n",
        "    w = w / w.sum()\n",
        "    out = np.zeros_like(probs_list[0], dtype=np.float32)\n",
        "    for wi, pi in zip(w, probs_list):\n",
        "        out += wi * pi\n",
        "    return out\n",
        "\n",
        "def metrics_from_probs(probs, y_true):\n",
        "    preds = probs.argmax(axis=1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, preds),\n",
        "        \"macro_f1\": f1_score(y_true, preds, average=\"macro\")\n",
        "    }\n",
        "\n",
        "def grid_search_weights_val(probs_list, y_true, step=0.05):\n",
        "    n = len(probs_list)\n",
        "    best = {\"weights\": None, \"macro_f1\": -1.0, \"accuracy\": 0.0}\n",
        "    grid = np.arange(0.0, 1.0 + 1e-9, step)\n",
        "\n",
        "    if n == 2:\n",
        "        for w0 in grid:\n",
        "            w = [w0, 1.0 - w0]\n",
        "            scores = metrics_from_probs(ensemble_probs(probs_list, w), y_true)\n",
        "            if scores[\"macro_f1\"] > best[\"macro_f1\"]:\n",
        "                best = {\"weights\": w, **scores}\n",
        "        return best\n",
        "\n",
        "    if n == 3:\n",
        "        for w0 in grid:\n",
        "            for w1 in grid:\n",
        "                s = w0 + w1\n",
        "                if s <= 1.0 + 1e-9:\n",
        "                    w2 = 1.0 - s\n",
        "                    w = [w0, w1, w2]\n",
        "                    scores = metrics_from_probs(ensemble_probs(probs_list, w), y_true)\n",
        "                    if scores[\"macro_f1\"] > best[\"macro_f1\"]:\n",
        "                        best = {\"weights\": w, **scores}\n",
        "        return best\n",
        "\n",
        "    raise ValueError(\"Only supports 2 or 3 models in this helper.\")\n",
        "\n",
        "# Lists for 3-model case (BERT + RoBERTa + DeBerta)\n",
        "val_probs_three_list  = [bert_val_probs, roberta_val_probs, deberta_val_probs]\n",
        "test_probs_three_list = [bert_test_probs, roberta_test_probs, deberta_test_probs]\n",
        "\n",
        "# Lists for 2-model case (BERT/RoBERTa/DeBerta)\n",
        "val_probs_two_list  = [bert_val_probs, deberta_val_probs]\n",
        "test_probs_two_list = [bert_test_probs, deberta_test_probs]\n",
        "\n",
        "# Grid search on validation\n",
        "best_three = grid_search_weights_val(val_probs_three_list, val_labels, step=0.05)\n",
        "print(\"3-Ensemble Best weights on validation:\", best_three)\n",
        "\n",
        "# Apply best weights to test\n",
        "ens_test_probs_three = ensemble_probs(test_probs_three_list, best_three[\"weights\"])\n",
        "test_scores_three = metrics_from_probs(ens_test_probs_three, test_labels)\n",
        "\n",
        "print(f\" Test (weighted 3-ensemble) Accuracy: {test_scores_three['accuracy']:.4f}\")\n",
        "print(f\" Test (weighted 3-ensemble) Macro-F1: {test_scores_three['macro_f1']:.4f}\")\n",
        "\n",
        "# Grid search on validation\n",
        "best_two = grid_search_weights_val(val_probs_two_list, val_labels, step=0.05)\n",
        "print(\"2-Ensemble Best weights on validation:\", best_two)\n",
        "\n",
        "# Apply best weights to test\n",
        "ens_test_probs_two = ensemble_probs(test_probs_two_list, best_two[\"weights\"])\n",
        "test_scores_two = metrics_from_probs(ens_test_probs_two, test_labels)\n",
        "\n",
        "print(f\" Test (weighted 2-ensemble) Accuracy: {test_scores_two['accuracy']:.4f}\")\n",
        "print(f\" Test (weighted 2-ensemble) Macro-F1: {test_scores_two['macro_f1']:.4f}\")"
      ],
      "metadata": {
        "id": "Bhn0TMNP5gvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import Javascript\n",
        "\n",
        "# def disconnect_runtime():\n",
        "#     display(Javascript('google.colab.kernel.disconnect()'))\n",
        "\n",
        "# disconnect_runtime()"
      ],
      "metadata": {
        "id": "kDtJ0GWh0I74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_85OMSltEjh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}